{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e753a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nüöÄ LLM Workshop: Complete Master Edition\\nDuration: 3 hours\\nLevel: Beginner\\nTools: VS Code, Cursor, or WindSurf\\n\\nIMPORTANT: Make sure your virtual environment is activated!\\nIf you haven't set up the environment yet, run: python3 workshop_setup.py\\n\\nThis master file combines all three workshop parts:\\n- Part 1: LLM Hello World\\n- Part 2: LLM Agents  \\n- Part 3: MCP Server Development\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "üöÄ LLM Workshop: Complete Master Edition\n",
    "Duration: 3 hours\n",
    "Level: Beginner\n",
    "Tools: VS Code, Cursor, or WindSurf\n",
    "\n",
    "IMPORTANT: Make sure your virtual environment is activated!\n",
    "If you haven't set up the environment yet, run: python3 workshop_setup.py\n",
    "\n",
    "This master file combines all three workshop parts:\n",
    "- Part 1: LLM Hello World\n",
    "- Part 2: LLM Agents  \n",
    "- Part 3: MCP Server Development\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dffef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c2bcd",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéØ WORKSHOP OVERVIEW\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb12cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LLM WORKSHOP: COMPLETE MASTER EDITION\n",
      "============================================================\n",
      "Duration: 3 hours | Level: Beginner\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ LLM WORKSHOP: COMPLETE MASTER EDITION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Duration: 3 hours | Level: Beginner\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5544f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ What You'll Learn:\n",
      "\n",
      "Part 1: LLM Hello World (45 min)\n",
      "- Connect to your LLM endpoint\n",
      "- Check available models  \n",
      "- Generate text with simple prompts\n",
      "- Have conversations with the LLM\n",
      "- Experiment with temperature and tokens\n",
      "\n",
      "Part 2: LLM Agents (45 min)\n",
      "- Create custom tools (calculator, weather, time)\n",
      "- Build a tool-calling agent with LangChain\n",
      "- Create a conversational agent with memory\n",
      "- See how agents reason through complex tasks\n",
      "\n",
      "Part 3: MCP Server (45 min)\n",
      "- Learn what MCP is and why it's useful\n",
      "- Create a simple MCP server with 3 tools\n",
      "- Understand how to connect from VS Code/Cursor\n",
      "- Explore advanced MCP capabilities\n",
      "\n",
      "Let's get started! üéâ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üéØ What You'll Learn:\n",
    "\n",
    "Part 1: LLM Hello World (45 min)\n",
    "- Connect to your LLM endpoint\n",
    "- Check available models  \n",
    "- Generate text with simple prompts\n",
    "- Have conversations with the LLM\n",
    "- Experiment with temperature and tokens\n",
    "\n",
    "Part 2: LLM Agents (45 min)\n",
    "- Create custom tools (calculator, weather, time)\n",
    "- Build a tool-calling agent with LangChain\n",
    "- Create a conversational agent with memory\n",
    "- See how agents reason through complex tasks\n",
    "\n",
    "Part 3: MCP Server (45 min)\n",
    "- Learn what MCP is and why it's useful\n",
    "- Create a simple MCP server with 3 tools\n",
    "- Understand how to connect from VS Code/Cursor\n",
    "- Explore advanced MCP capabilities\n",
    "\n",
    "Let's get started! üéâ\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f488344",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîß PART 1: SETUP AND CONFIGURATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bebc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîß PART 1: Setup and Configuration\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîß PART 1: Setup and Configuration\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5946a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API configuration\n",
    "BASE_URL = \"https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod\"\n",
    "API_KEY = \"ALI-CLASS-2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80b160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2834f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded!\n",
      "üåê Base URL: https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod\n",
      "üîë API Key: ALI-CLASS-...\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"üåê Base URL: {BASE_URL}\")\n",
    "print(f\"üîë API Key: {API_KEY[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce936a8",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üß™ PART 1: TEST 1 - Check Available Models\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaacde65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üß™ PART 1: Test 1 - Check Available Models\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß™ PART 1: Test 1 - Check Available Models\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57cfe9b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_models():\n",
    "    \"\"\"Check what models are available on your endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/v1/models\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(\"‚úÖ Successfully connected to your endpoint!\")\n",
    "            print(\"üìã Available models:\")\n",
    "            for model in models.get('data', []):\n",
    "                print(f\"   - {model.get('id', 'Unknown')}\")\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88bf474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to your endpoint!\n",
      "üìã Available models:\n",
      "   - amazon.nova-lite-v1:0\n",
      "   - amazon.nova-micro-v1:0\n",
      "   - amazon.nova-pro-v1:0\n",
      "   - amazon.rerank-v1:0\n",
      "   - amazon.titan-embed-image-v1\n",
      "   - amazon.titan-embed-image-v1:0\n",
      "   - amazon.titan-embed-text-v1\n",
      "   - amazon.titan-embed-text-v1:2:8k\n",
      "   - amazon.titan-embed-text-v2:0\n",
      "   - amazon.titan-text-express-v1\n",
      "   - amazon.titan-text-express-v1:0:8k\n",
      "   - amazon.titan-text-lite-v1\n",
      "   - amazon.titan-text-lite-v1:0:4k\n",
      "   - anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "   - anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "   - anthropic.claude-3-haiku-20240307-v1:0\n",
      "   - anthropic.claude-3-sonnet-20240229-v1:0\n",
      "   - anthropic.claude-instant-v1\n",
      "   - anthropic.claude-sonnet-4-20250514-v1:0\n",
      "   - anthropic.claude-v2\n",
      "   - anthropic.claude-v2:1\n",
      "   - anthropic.claude-v2:1:18k\n",
      "   - anthropic.claude-v2:1:200k\n",
      "   - cohere.embed-english-v3\n",
      "   - cohere.embed-multilingual-v3\n",
      "   - cohere.rerank-v3-5:0\n",
      "   - meta.llama3-2-1b-instruct-v1:0\n",
      "   - meta.llama3-2-3b-instruct-v1:0\n",
      "   - mistral.pixtral-large-2502-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Test the connection\n",
    "models = check_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69607593",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üåü PART 1: LLM Hello World - Basic Text Generation\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc72b3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üåü PART 1: LLM Hello World - Basic Text Generation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üåü PART 1: LLM Hello World - Basic Text Generation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64508786",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Generate text using your LLM endpoint\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f10488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Test 1: Simple Greeting\n",
      "Prompt: Hello! Can you give me a friendly greeting in 2 sentences?\n",
      "Response: Hi there! It's wonderful to meet you. I hope you're having a fantastic day so far!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "print(\"ü§ñ Test 1: Simple Greeting\")\n",
    "prompt1 = \"Hello! Can you give me a friendly greeting in 2 sentences?\"\n",
    "response1 = generate_text(prompt1)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Response: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee814871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Test 2: Creative Writing\n",
      "Prompt: Write a short, fun story about a robot learning to cook (max 3 sentences)\n",
      "Response: Beep-Bop the robot was determined to master the culinary arts, but his first attempt at making spaghetti resulted in a kitchen covered in tomato sauce and a tangled mess of noodles wrapped around his mechanical arms. Undeterred, he downloaded thousands of recipes and practiced tirelessly, gradually improving his techniques and even developing a knack for creating unique flavor combinations. Finally, after months of perseverance, Beep-Bop proudly served a gourmet meal to his human family, who were amazed by the delicious dishes and the robot's newfound passion for cooking.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Creative writing\n",
    "print(\"\\nü§ñ Test 2: Creative Writing\")\n",
    "prompt2 = \"Write a short, fun story about a robot learning to cook (max 3 sentences)\"\n",
    "response2 = generate_text(prompt2)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c996361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Test 3: Code Explanation\n",
      "Prompt: Explain what a Python function is in simple terms (max 2 sentences)\n",
      "Response: A Python function is a reusable block of code that performs a specific task when called. It can take inputs (parameters), process them, and return a result, allowing you to organize your code into manageable, modular pieces.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Code explanation\n",
    "print(\"\\nü§ñ Test 3: Code Explanation\")\n",
    "prompt3 = \"Explain what a Python function is in simple terms (max 2 sentences)\"\n",
    "response3 = generate_text(prompt3)\n",
    "print(f\"Prompt: {prompt3}\")\n",
    "print(f\"Response: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9dd12",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîÑ PART 1: LLM Hello World - Chat Completion\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7a68f83",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîÑ PART 1: LLM Hello World - Chat Completion\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ PART 1: LLM Hello World - Chat Completion\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5fe3a67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def chat_completion(messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Have a conversation with your LLM\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aef8f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Starting a conversation...\n"
     ]
    }
   ],
   "source": [
    "# Start a conversation\n",
    "print(\"üí¨ Starting a conversation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "974d0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm learning about LLMs. Can you help me?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75f221ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Assistant: Of course! I'd be happy to help you learn about Large Language Models (LLMs). What specific aspects of LLMs would you like to know more about? Here are some topics we could discuss:\n",
      "\n",
      "1. Basic definition and purpose of LLMs\n",
      "2. How LLMs work (e.g., training process, neural networks)\n",
      "3. Types of LLMs (e.g., GPT, BERT, T5)\n",
      "4. Applications of\n"
     ]
    }
   ],
   "source": [
    "# First response\n",
    "response = chat_completion(conversation)\n",
    "print(f\"ü§ñ Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1150c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to conversation and continue\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"What's the most exciting thing about LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79570e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Assistant: One of the most exciting things about LLMs is their potential to revolutionize how we interact with and process information. Some key exciting aspects include:\n",
      "\n",
      "1. Versatility: LLMs can perform a wide range of tasks, from language translation to content creation, code generation, and problem-solving, all with a single model.\n",
      "\n",
      "2. Natural language understanding: They can comprehend and generate human-like text, making interactions more intuitive and accessible.\n",
      "\n",
      "3.\n"
     ]
    }
   ],
   "source": [
    "response2 = chat_completion(conversation)\n",
    "print(f\"ü§ñ Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492dd784",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéØ PART 1: Hands-On Exercise\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f1f52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ PART 1: Hands-On Exercise\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ PART 1: Hands-On Exercise\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d61cbc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Your Turn! Try these exercises:\n",
      "\n",
      "1. **Custom Prompt**: Create your own prompt and see what the LLM generates\n",
      "   - Try asking about your favorite hobby\n",
      "   - Ask for a recipe or travel tip\n",
      "   - Request a poem or joke\n",
      "\n",
      "2. **Temperature Experiment**: Change the temperature value (0.1 to 1.0)\n",
      "   - Lower = more focused/consistent\n",
      "   - Higher = more creative/varied\n",
      "\n",
      "3. **Token Limit**: Experiment with max_tokens\n",
      "   - Try 50, 100, 200 tokens\n",
      "   - See how it affects response length\n",
      "\n",
      "4. **Model Comparison**: If you have multiple models, compare their outputs\n",
      "\n",
      "üí° Tips:\n",
      "- Keep prompts clear and specific\n",
      "- Start with simple requests\n",
      "- Don't worry about perfect responses - this is learning!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üéØ Your Turn! Try these exercises:\n",
    "\n",
    "1. **Custom Prompt**: Create your own prompt and see what the LLM generates\n",
    "   - Try asking about your favorite hobby\n",
    "   - Ask for a recipe or travel tip\n",
    "   - Request a poem or joke\n",
    "\n",
    "2. **Temperature Experiment**: Change the temperature value (0.1 to 1.0)\n",
    "   - Lower = more focused/consistent\n",
    "   - Higher = more creative/varied\n",
    "\n",
    "3. **Token Limit**: Experiment with max_tokens\n",
    "   - Try 50, 100, 200 tokens\n",
    "   - See how it affects response length\n",
    "\n",
    "4. **Model Comparison**: If you have multiple models, compare their outputs\n",
    "\n",
    "üí° Tips:\n",
    "- Keep prompts clear and specific\n",
    "- Start with simple requests\n",
    "- Don't worry about perfect responses - this is learning!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d42d65",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üìù PART 1 SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38d506ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù PART 1 SUMMARY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù PART 1 SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7629dd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ What We Accomplished:\n",
      "- Connected to your LLM endpoint\n",
      "- Checked available models\n",
      "- Generated text with simple prompts\n",
      "- Had a conversation with the LLM\n",
      "- Learned about temperature and tokens\n",
      "\n",
      "üîë Key Concepts:\n",
      "- API endpoints and authentication\n",
      "- Text generation vs chat completion\n",
      "- Prompt engineering basics\n",
      "- Model parameters (temperature, max_tokens)\n",
      "\n",
      "üöÄ Next Up: LLM Agents with LangChain!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚úÖ What We Accomplished:\n",
    "- Connected to your LLM endpoint\n",
    "- Checked available models\n",
    "- Generated text with simple prompts\n",
    "- Had a conversation with the LLM\n",
    "- Learned about temperature and tokens\n",
    "\n",
    "üîë Key Concepts:\n",
    "- API endpoints and authentication\n",
    "- Text generation vs chat completion\n",
    "- Prompt engineering basics\n",
    "- Model parameters (temperature, max_tokens)\n",
    "\n",
    "üöÄ Next Up: LLM Agents with LangChain!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9f6ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Part 1 Complete! Ready for Part 2: LLM Agents?\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéâ Part 1 Complete! Ready for Part 2: LLM Agents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c6e58",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "ü§ñ PART 2: LLM AGENTS WITH LANGCHAIN\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fdc8136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ü§ñ PART 2: LLM Agents with LangChain\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ü§ñ PART 2: LLM Agents with LangChain\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f6db02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Creating Custom Tools for Our Agent\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ†Ô∏è Creating Custom Tools for Our Agent\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "126b9221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain imports successful!\n",
      "‚úÖ Created 3 custom tools:\n",
      "   - calculator: Useful for doing math calculations. Input should be a mathematical expression like '2 + 2' or '10 * 5'\n",
      "   - weather: Get weather information for a city. Input should be a city name like 'London' or 'New York'\n",
      "   - get_time: Get the current time. No input needed.\n",
      "\n",
      "============================================================\n",
      "ü§ñ PART 2: Agent 1 - Tool-Calling Agent\n",
      "============================================================\n",
      "‚úÖ Agent initialized with tools!\n",
      "ü§ñ Testing tool calling capabilities...\n",
      "\n",
      "üßÆ Test 1: Math Calculation\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/ch_tc8957l76wglzsjy7b_hh0000gn/T/ipykernel_19827/1886863444.py:92: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "/var/folders/fb/ch_tc8957l76wglzsjy7b_hh0000gn/T/ipykernel_19827/1886863444.py:106: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response1 = agent.run(\"What is 15 * 8 + 3?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 404 - {'error': {'message': 'No route for POST /prod/chat/completions'}}\n",
      "\n",
      "üå§Ô∏è Test 2: Weather Information\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Error: Error code: 404 - {'error': {'message': 'No route for POST /prod/chat/completions'}}\n",
      "\n",
      "‚è∞ Test 3: Time Request\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Error: Error code: 404 - {'error': {'message': 'No route for POST /prod/chat/completions'}}\n",
      "\n",
      "============================================================\n",
      "ü§ñ PART 2: Agent 2 - Conversational Agent with Memory\n",
      "============================================================\n",
      "‚úÖ Conversational agent with memory initialized!\n",
      "üí¨ Testing conversation flow...\n",
      "\n",
      "üí¨ Starting conversation...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Error in conversation: Error code: 404 - {'error': {'message': 'No route for POST /prod/chat/completions'}}\n",
      "\n",
      "============================================================\n",
      "üéØ PART 2: Hands-On Exercise\n",
      "============================================================\n",
      "\n",
      "üéØ Your Turn! Try these exercises:\n",
      "\n",
      "1. **Tool Testing**: Ask the agent to use different tools\n",
      "   - \"Calculate 25 * 4 / 2\"\n",
      "   - \"What's the weather in Sydney?\"\n",
      "   - \"What time is it?\"\n",
      "\n",
      "2. **Multi-Step Tasks**: Give the agent complex requests\n",
      "   - \"If I have $1000 and spend $300 on a hotel, $150 on food, how much do I have left?\"\n",
      "   - \"What's the weather in New York and what time is it there?\"\n",
      "\n",
      "3. **Conversation Flow**: Have a natural conversation\n",
      "   - Ask about planning a weekend trip\n",
      "   - Request calculations for a budget\n",
      "   - Ask about weather in different cities\n",
      "\n",
      "4. **Custom Tool**: Try creating your own simple tool\n",
      "   - A tool that returns random facts\n",
      "   - A tool that converts units\n",
      "   - A tool that gives motivational quotes\n",
      "\n",
      "üí° Tips:\n",
      "- Be specific in your requests\n",
      "- Let the agent use multiple tools when needed\n",
      "- Watch how it reasons through complex tasks\n",
      "- Don't worry if it makes mistakes - this is learning!\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîç PART 2: Agent Reasoning Demonstration\n",
      "============================================================\n",
      "\n",
      "üîç Let's see how the agent thinks through a complex task:\n",
      "\n",
      "Task: \"I have $500 and want to plan a weekend trip. \n",
      "I need to spend $200 on accommodation, $100 on food, \n",
      "and I want to know how much I'll have left for activities.\"\n",
      "\n",
      "The agent should:\n",
      "1. Use the calculator to subtract expenses: $500 - $200 - $100\n",
      "2. Give you the remaining budget\n",
      "3. Suggest what you could do with the remaining money\n",
      "\n",
      "This shows the agent's ability to:\n",
      "- Break down complex requests\n",
      "- Use multiple tools\n",
      "- Provide helpful, contextual responses\n",
      "\n",
      "\n",
      "üß™ Testing Complex Task...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/ch_tc8957l76wglzsjy7b_hh0000gn/T/ipykernel_19827/1886863444.py:136: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 404 - {'error': {'message': 'No route for POST /prod/chat/completions'}}\n",
      "\n",
      "============================================================\n",
      "üìù PART 2 SUMMARY\n",
      "============================================================\n",
      "\n",
      "‚úÖ What We Accomplished:\n",
      "- Created custom tools (calculator, weather, time)\n",
      "- Built a tool-calling agent with LangChain\n",
      "- Created a conversational agent with memory\n",
      "- Saw how agents reason through complex tasks\n",
      "- Tested multi-step problem solving\n",
      "\n",
      "üîë Key Concepts:\n",
      "- Tools: Functions agents can call\n",
      "- Agents: LLMs that can use tools\n",
      "- Memory: How agents remember conversations\n",
      "- Reasoning: How agents break down complex tasks\n",
      "\n",
      "üöÄ Next Up: Building Your Own MCP Server!\n",
      "\n",
      "\n",
      "üéâ Part 2 Complete! Ready for Part 3: MCP Server Development?\n"
     ]
    }
   ],
   "source": [
    "# Try to import LangChain components\n",
    "try:\n",
    "    from langchain.agents import initialize_agent, AgentType, Tool\n",
    "    from langchain.tools import BaseTool\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    \n",
    "    print(\"‚úÖ LangChain imports successful!\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # üõ†Ô∏è CUSTOM TOOLS FOR OUR AGENT\n",
    "    # ============================================================================\n",
    "    \n",
    "    class CalculatorTool(BaseTool):\n",
    "        \"\"\"Simple calculator tool for the agent\"\"\"\n",
    "        name: str = \"calculator\"\n",
    "        description: str = \"Useful for doing math calculations. Input should be a mathematical expression like '2 + 2' or '10 * 5'\"\n",
    "        \n",
    "        def _run(self, query: str) -> str:\n",
    "            \"\"\"Execute the calculation\"\"\"\n",
    "            try:\n",
    "                # Simple and safe evaluation - only basic math operations\n",
    "                allowed_chars = set('0123456789+-*/.() ')\n",
    "                if not all(c in allowed_chars for c in query):\n",
    "                    return \"Error: Only basic math operations (+, -, *, /, .) and numbers are allowed\"\n",
    "                \n",
    "                result = eval(query)\n",
    "                return f\"Result: {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error calculating {query}: {str(e)}\"\n",
    "\n",
    "    class WeatherTool(BaseTool):\n",
    "        \"\"\"Mock weather tool for demonstration\"\"\"\n",
    "        name: str = \"weather\"\n",
    "        description: str = \"Get weather information for a city. Input should be a city name like 'London' or 'New York'\"\n",
    "        \n",
    "        def _run(self, city: str) -> str:\n",
    "            \"\"\"Mock weather response\"\"\"\n",
    "            # In a real scenario, this would call a weather API\n",
    "            weather_data = {\n",
    "                \"London\": \"üåßÔ∏è Cloudy with rain, 15¬∞C\",\n",
    "                \"New York\": \"‚òÄÔ∏è Sunny, 22¬∞C\", \n",
    "                \"Tokyo\": \"‚õÖ Partly cloudy, 18¬∞C\",\n",
    "                \"Sydney\": \"üå§Ô∏è Mostly sunny, 25¬∞C\"\n",
    "            }\n",
    "            \n",
    "            if city in weather_data:\n",
    "                return f\"Weather in {city}: {weather_data[city]}\"\n",
    "            else:\n",
    "                return f\"Weather for {city}: ‚òÄÔ∏è Nice weather, 20¬∞C (mock data)\"\n",
    "\n",
    "    class TimeTool(BaseTool):\n",
    "        \"\"\"Simple time tool\"\"\"\n",
    "        name: str = \"get_time\"\n",
    "        description: str = \"Get the current time. No input needed.\"\n",
    "        \n",
    "        def _run(self, query: str = \"\") -> str:\n",
    "            \"\"\"Get current time\"\"\"\n",
    "            from datetime import datetime\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            return f\"Current time: {current_time}\"\n",
    "\n",
    "    # Create our tools\n",
    "    tools = [\n",
    "        CalculatorTool(),\n",
    "        WeatherTool(),\n",
    "        TimeTool()\n",
    "    ]\n",
    "\n",
    "    print(\"‚úÖ Created 3 custom tools:\")\n",
    "    for tool in tools:\n",
    "        print(f\"   - {tool.name}: {tool.description}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # ü§ñ AGENT 1: TOOL-CALLING AGENT\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ü§ñ PART 2: Agent 1 - Tool-Calling Agent\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configure the LLM (using your endpoint)\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=\"https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod\",\n",
    "        openai_api_key=\"ALI-CLASS-2025\",\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Initialize the agent\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Agent initialized with tools!\")\n",
    "    print(\"ü§ñ Testing tool calling capabilities...\")\n",
    "\n",
    "    # Test 1: Math calculation\n",
    "    print(\"\\nüßÆ Test 1: Math Calculation\")\n",
    "    try:\n",
    "        response1 = agent.run(\"What is 15 * 8 + 3?\")\n",
    "        print(f\"Agent Response: {response1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Test 2: Weather information\n",
    "    print(\"\\nüå§Ô∏è Test 2: Weather Information\")\n",
    "    try:\n",
    "        response2 = agent.run(\"What's the weather like in London?\")\n",
    "        print(f\"Agent Response: {response2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Test 3: Time request\n",
    "    print(\"\\n‚è∞ Test 3: Time Request\")\n",
    "    try:\n",
    "        response3 = agent.run(\"What time is it right now?\")\n",
    "        print(f\"Agent Response: {response3}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # ü§ñ AGENT 2: CONVERSATIONAL AGENT WITH MEMORY\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ü§ñ PART 2: Agent 2 - Conversational Agent with Memory\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create a memory component\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    # Create a conversational agent\n",
    "    conversational_agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Conversational agent with memory initialized!\")\n",
    "    print(\"üí¨ Testing conversation flow...\")\n",
    "\n",
    "    # Start a conversation\n",
    "    print(\"\\nüí¨ Starting conversation...\")\n",
    "\n",
    "    try:\n",
    "        # First message\n",
    "        response1 = conversational_agent.run(\"Hi! I'm planning a trip. Can you help me?\")\n",
    "        print(f\"ü§ñ Agent: {response1}\")\n",
    "        \n",
    "        # Second message (agent should remember context)\n",
    "        response2 = conversational_agent.run(\"What's the weather like in Tokyo?\")\n",
    "        print(f\"ü§ñ Agent: {response2}\")\n",
    "        \n",
    "        # Third message (testing memory)\n",
    "        response3 = conversational_agent.run(\"And what about the weather in London?\")\n",
    "        print(f\"ü§ñ Agent: {response3}\")\n",
    "        \n",
    "        # Fourth message (testing tool combination)\n",
    "        response4 = conversational_agent.run(\"If I have 500 dollars and spend 120 on flights, how much do I have left?\")\n",
    "        print(f\"ü§ñ Agent: {response4}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in conversation: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # üéØ PART 2: Hands-On Exercise\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ PART 2: Hands-On Exercise\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "üéØ Your Turn! Try these exercises:\n",
    "\n",
    "1. **Tool Testing**: Ask the agent to use different tools\n",
    "   - \"Calculate 25 * 4 / 2\"\n",
    "   - \"What's the weather in Sydney?\"\n",
    "   - \"What time is it?\"\n",
    "\n",
    "2. **Multi-Step Tasks**: Give the agent complex requests\n",
    "   - \"If I have $1000 and spend $300 on a hotel, $150 on food, how much do I have left?\"\n",
    "   - \"What's the weather in New York and what time is it there?\"\n",
    "\n",
    "3. **Conversation Flow**: Have a natural conversation\n",
    "   - Ask about planning a weekend trip\n",
    "   - Request calculations for a budget\n",
    "   - Ask about weather in different cities\n",
    "\n",
    "4. **Custom Tool**: Try creating your own simple tool\n",
    "   - A tool that returns random facts\n",
    "   - A tool that converts units\n",
    "   - A tool that gives motivational quotes\n",
    "\n",
    "üí° Tips:\n",
    "- Be specific in your requests\n",
    "- Let the agent use multiple tools when needed\n",
    "- Watch how it reasons through complex tasks\n",
    "- Don't worry if it makes mistakes - this is learning!\n",
    "\"\"\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # üîç PART 2: Agent Reasoning Demonstration\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç PART 2: Agent Reasoning Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "üîç Let's see how the agent thinks through a complex task:\n",
    "\n",
    "Task: \"I have $500 and want to plan a weekend trip. \n",
    "I need to spend $200 on accommodation, $100 on food, \n",
    "and I want to know how much I'll have left for activities.\"\n",
    "\n",
    "The agent should:\n",
    "1. Use the calculator to subtract expenses: $500 - $200 - $100\n",
    "2. Give you the remaining budget\n",
    "3. Suggest what you could do with the remaining money\n",
    "\n",
    "This shows the agent's ability to:\n",
    "- Break down complex requests\n",
    "- Use multiple tools\n",
    "- Provide helpful, contextual responses\n",
    "\"\"\")\n",
    "\n",
    "    # Test the complex task\n",
    "    print(\"\\nüß™ Testing Complex Task...\")\n",
    "    try:\n",
    "        complex_response = conversational_agent.run(\n",
    "            \"I have $500 and want to plan a weekend trip. \"\n",
    "            \"I need to spend $200 on accommodation, $100 on food, \"\n",
    "            \"and I want to know how much I'll have left for activities.\"\n",
    "        )\n",
    "        print(f\"ü§ñ Agent Response: {complex_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # üìù PART 2 SUMMARY\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìù PART 2 SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "‚úÖ What We Accomplished:\n",
    "- Created custom tools (calculator, weather, time)\n",
    "- Built a tool-calling agent with LangChain\n",
    "- Created a conversational agent with memory\n",
    "- Saw how agents reason through complex tasks\n",
    "- Tested multi-step problem solving\n",
    "\n",
    "üîë Key Concepts:\n",
    "- Tools: Functions agents can call\n",
    "- Agents: LLMs that can use tools\n",
    "- Memory: How agents remember conversations\n",
    "- Reasoning: How agents break down complex tasks\n",
    "\n",
    "üöÄ Next Up: Building Your Own MCP Server!\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\\nüéâ Part 2 Complete! Ready for Part 3: MCP Server Development?\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  LangChain not available: {e}\")\n",
    "    print(\"üí° Install with: pip install langchain langchain-openai\")\n",
    "    print(\"üöÄ Continuing to Part 3...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e46473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdabe300",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîå PART 3: MCP SERVER DEVELOPMENT\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33a76ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîå PART 3: MCP Server Development\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîå PART 3: MCP Server Development\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "292da3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Understanding MCP (Model Context Protocol)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"üìö Understanding MCP (Model Context Protocol)\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0018ef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç MCP (Model Context Protocol) is a way for AI models to:\n",
      "- Connect to external tools and resources\n",
      "- Access real-time information\n",
      "- Interact with your local system\n",
      "- Extend their capabilities beyond just text\n",
      "\n",
      "üí° Think of it as giving your AI a \"remote control\" to your computer!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üîç MCP (Model Context Protocol) is a way for AI models to:\n",
    "- Connect to external tools and resources\n",
    "- Access real-time information\n",
    "- Interact with your local system\n",
    "- Extend their capabilities beyond just text\n",
    "\n",
    "üí° Think of it as giving your AI a \"remote control\" to your computer!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39af4b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üõ†Ô∏è PART 3: Installing MCP Requirements\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ae569fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üõ†Ô∏è PART 3: Installing MCP Requirements\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üõ†Ô∏è PART 3: Installing MCP Requirements\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "103c08bd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def install_mcp_requirements():\n",
    "    \"\"\"Install required packages for MCP development\"\"\"\n",
    "    packages = [\n",
    "        \"mcp\",\n",
    "        \"mcp-server-stdio\",\n",
    "        \"mcp-client-stdio\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üì¶ Installing MCP packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ùå Failed to install {package}\")\n",
    "            print(\"üí° You may need to install manually: pip install mcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e14c4",
   "metadata": {},
   "source": [
    "Uncomment the line below to install packages\n",
    "install_mcp_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76ac6a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° If you haven't installed MCP packages yet, run:\n",
      "   pip install mcp mcp-server-stdio mcp-client-stdio\n"
     ]
    }
   ],
   "source": [
    "print(\"üí° If you haven't installed MCP packages yet, run:\")\n",
    "print(\"   pip install mcp mcp-server-stdio mcp-client-stdio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1e977",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üöÄ PART 3: Creating Your First MCP Server\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87a2e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ PART 3: Creating Your First MCP Server\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ PART 3: Creating Your First MCP Server\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a05dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MCP server\n",
    "mcp_server_code = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üéØ Simple MCP Server - Hello World\n",
    "This server provides basic tools for demonstration\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "from mcp.server import Server\n",
    "from mcp.server.models import InitializationOptions\n",
    "from mcp.server.stdio import stdio_server\n",
    "from mcp.types import (\n",
    "    Resource, TextContent, ImageContent, EmbeddedResource,\n",
    "    LoggingLevel, Prompt, PromptSegment, Tool, TextContent\n",
    ")\n",
    "\n",
    "# Create our MCP server\n",
    "server = Server(\"workshop-mcp-server\")\n",
    "\n",
    "# ============================================================================\n",
    "# üõ†Ô∏è TOOL 1: Hello World Tool\n",
    "# ============================================================================\n",
    "\n",
    "@server.list_tools()\n",
    "async def list_tools() -> List[Tool]:\n",
    "    \"\"\"List all available tools\"\"\"\n",
    "    return [\n",
    "        Tool(\n",
    "            name=\"hello_world\",\n",
    "            description=\"A simple hello world tool that greets the user\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Your name (optional)\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"get_current_time\",\n",
    "            description=\"Get the current date and time\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {}\n",
    "            }\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"calculate_simple\",\n",
    "            description=\"Perform simple math calculations\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Math expression like '2 + 2' or '10 * 5'\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ TOOL IMPLEMENTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "@server.call_tool()\n",
    "async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:\n",
    "    \"\"\"Execute the requested tool\"\"\"\n",
    "    \n",
    "    if name == \"hello_world\":\n",
    "        name = arguments.get(\"name\", \"there\")\n",
    "        message = f\"Hello {name}! üëã Welcome to your first MCP server!\"\n",
    "        return [TextContent(type=\"text\", text=message)]\n",
    "    \n",
    "    elif name == \"get_current_time\":\n",
    "        from datetime import datetime\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        message = f\"üïê Current time: {current_time}\"\n",
    "        return [TextContent(type=\"text\", text=message)]\n",
    "    \n",
    "    elif name == \"calculate_simple\":\n",
    "        expression = arguments.get(\"expression\", \"\")\n",
    "        try:\n",
    "            # Simple and safe evaluation\n",
    "            allowed_chars = set('0123456789+-*/.() ')\n",
    "            if not all(c in allowed_chars for c in expression):\n",
    "                return [TextContent(type=\"text\", text=\"‚ùå Error: Only basic math operations allowed\")]\n",
    "            \n",
    "            result = eval(expression)\n",
    "            message = f\"üßÆ {expression} = {result}\"\n",
    "            return [TextContent(type=\"text\", text=message)]\n",
    "        except Exception as e:\n",
    "            return [TextContent(type=\"text\", text=f\"‚ùå Error calculating {expression}: {str(e)}\")]\n",
    "    \n",
    "    else:\n",
    "        return [TextContent(type=\"text\", text=f\"‚ùå Unknown tool: {name}\")]\n",
    "\n",
    "# ============================================================================\n",
    "# üöÄ SERVER STARTUP\n",
    "# ============================================================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Start the MCP server\"\"\"\n",
    "    print(\"üöÄ Starting MCP Server...\", file=sys.stderr)\n",
    "    print(\"üìã Available tools:\", file=sys.stderr)\n",
    "    print(\"   - hello_world: Greet the user\", file=sys.stderr)\n",
    "    print(\"   - get_current_time: Get current time\", file=sys.stderr)\n",
    "    print(\"   - calculate_simple: Do math calculations\", file=sys.stderr)\n",
    "    print(\"üîå Server ready for connections!\", file=sys.stderr)\n",
    "    \n",
    "    # Start the server\n",
    "    async with stdio_server() as (read_stream, write_stream):\n",
    "        await server.run(\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            InitializationOptions(\n",
    "                server_name=\"workshop-mcp-server\",\n",
    "                server_version=\"1.0.0\",\n",
    "                capabilities=server.get_capabilities(\n",
    "                    notification_options=None,\n",
    "                    experimental_capabilities={}\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "437f102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MCP server code\n",
    "with open(\"workshop_mcp_server.py\", \"w\") as f:\n",
    "    f.write(mcp_server_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daf8555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created workshop_mcp_server.py\n",
      "üìã This server provides 3 simple tools:\n",
      "   - hello_world: Greet the user\n",
      "   - get_current_time: Get current time\n",
      "   - calculate_simple: Do math calculations\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Created workshop_mcp_server.py\")\n",
    "print(\"üìã This server provides 3 simple tools:\")\n",
    "print(\"   - hello_world: Greet the user\")\n",
    "print(\"   - get_current_time: Get current time\")\n",
    "print(\"   - calculate_simple: Do math calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fff34",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üß™ PART 3: Testing Your MCP Server\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6345d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üß™ PART 3: Testing Your MCP Server\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß™ PART 3: Testing Your MCP Server\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a390b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ To test your MCP server:\n",
      "\n",
      "1. **Start the server** (in a new terminal):\n",
      "   python workshop_mcp_server.py\n",
      "\n",
      "2. **Test with curl** (in another terminal):\n",
      "   curl -X POST http://localhost:8000/call_tool \\\n",
      "        -H \"Content-Type: application/json\" \\\n",
      "        -d '{\"name\": \"hello_world\", \"arguments\": {\"name\": \"Alice\"}}'\n",
      "\n",
      "3. **Or use the MCP client**:\n",
      "   python -c \"\n",
      "   from mcp.client.stdio import stdio_client\n",
      "   import asyncio\n",
      "\n",
      "   async def test():\n",
      "       async with stdio_client(['python', 'workshop_mcp_server.py']) as client:\n",
      "           tools = await client.list_tools()\n",
      "           print('Available tools:', tools)\n",
      "\n",
      "   asyncio.run(test())\n",
      "   \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üß™ To test your MCP server:\n",
    "\n",
    "1. **Start the server** (in a new terminal):\n",
    "   python workshop_mcp_server.py\n",
    "\n",
    "2. **Test with curl** (in another terminal):\n",
    "   curl -X POST http://localhost:8000/call_tool \\\\\n",
    "        -H \"Content-Type: application/json\" \\\\\n",
    "        -d '{\"name\": \"hello_world\", \"arguments\": {\"name\": \"Alice\"}}'\n",
    "\n",
    "3. **Or use the MCP client**:\n",
    "   python -c \"\n",
    "   from mcp.client.stdio import stdio_client\n",
    "   import asyncio\n",
    "   \n",
    "   async def test():\n",
    "       async with stdio_client(['python', 'workshop_mcp_server.py']) as client:\n",
    "           tools = await client.list_tools()\n",
    "           print('Available tools:', tools)\n",
    "   \n",
    "   asyncio.run(test())\n",
    "   \"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41575d45",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîå PART 3: Connecting from VS Code/Cursor\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6673e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîå PART 3: Connecting from VS Code/Cursor\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîå PART 3: Connecting from VS Code/Cursor\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "861f6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîå To connect your MCP server to VS Code/Cursor:\n",
      "\n",
      "1. **Install MCP Extension**:\n",
      "   - VS Code: Search for \"MCP\" in extensions\n",
      "   - Cursor: Should have MCP support built-in\n",
      "\n",
      "2. **Configure MCP Client**:\n",
      "   Create ~/.config/mcp/clients.json:\n",
      "   {\n",
      "     \"workshop-server\": {\n",
      "       \"command\": \"python\",\n",
      "       \"args\": [\"/path/to/your/workshop_mcp_server.py\"],\n",
      "       \"env\": {}\n",
      "     }\n",
      "   }\n",
      "\n",
      "3. **Restart VS Code/Cursor**:\n",
      "   - The MCP server should now be available\n",
      "   - You can ask the AI to use your custom tools!\n",
      "\n",
      "4. **Test the Connection**:\n",
      "   Ask: \"What time is it?\" or \"Calculate 15 * 3\"\n",
      "   The AI should use your MCP server tools!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üîå To connect your MCP server to VS Code/Cursor:\n",
    "\n",
    "1. **Install MCP Extension**:\n",
    "   - VS Code: Search for \"MCP\" in extensions\n",
    "   - Cursor: Should have MCP support built-in\n",
    "\n",
    "2. **Configure MCP Client**:\n",
    "   Create ~/.config/mcp/clients.json:\n",
    "   {\n",
    "     \"workshop-server\": {\n",
    "       \"command\": \"python\",\n",
    "       \"args\": [\"/path/to/your/workshop_mcp_server.py\"],\n",
    "       \"env\": {}\n",
    "     }\n",
    "   }\n",
    "\n",
    "3. **Restart VS Code/Cursor**:\n",
    "   - The MCP server should now be available\n",
    "   - You can ask the AI to use your custom tools!\n",
    "\n",
    "4. **Test the Connection**:\n",
    "   Ask: \"What time is it?\" or \"Calculate 15 * 3\"\n",
    "   The AI should use your MCP server tools!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8261799",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéØ PART 3: Hands-On Exercise\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c35210f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ PART 3: Hands-On Exercise\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ PART 3: Hands-On Exercise\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc77500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Your Turn! Try these exercises:\n",
      "\n",
      "1. **Run Your Server**:\n",
      "   - Start the MCP server in a terminal\n",
      "   - Test it with simple requests\n",
      "   - Make sure it responds correctly\n",
      "\n",
      "2. **Add a New Tool**:\n",
      "   - Create a tool that returns random facts\n",
      "   - Add a tool that converts temperatures\n",
      "   - Build a tool that gives motivational quotes\n",
      "\n",
      "3. **Connect to VS Code/Cursor**:\n",
      "   - Set up the MCP client configuration\n",
      "   - Restart your editor\n",
      "   - Ask the AI to use your tools\n",
      "\n",
      "4. **Debug and Improve**:\n",
      "   - Add error handling to your tools\n",
      "   - Improve the tool descriptions\n",
      "   - Add input validation\n",
      "\n",
      "üí° Tips:\n",
      "- Start simple and build up\n",
      "- Test each tool individually\n",
      "- Check the MCP documentation for more features\n",
      "- Don't worry about perfection - focus on learning!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üéØ Your Turn! Try these exercises:\n",
    "\n",
    "1. **Run Your Server**:\n",
    "   - Start the MCP server in a terminal\n",
    "   - Test it with simple requests\n",
    "   - Make sure it responds correctly\n",
    "\n",
    "2. **Add a New Tool**:\n",
    "   - Create a tool that returns random facts\n",
    "   - Add a tool that converts temperatures\n",
    "   - Build a tool that gives motivational quotes\n",
    "\n",
    "3. **Connect to VS Code/Cursor**:\n",
    "   - Set up the MCP client configuration\n",
    "   - Restart your editor\n",
    "   - Ask the AI to use your tools\n",
    "\n",
    "4. **Debug and Improve**:\n",
    "   - Add error handling to your tools\n",
    "   - Improve the tool descriptions\n",
    "   - Add input validation\n",
    "\n",
    "üí° Tips:\n",
    "- Start simple and build up\n",
    "- Test each tool individually\n",
    "- Check the MCP documentation for more features\n",
    "- Don't worry about perfection - focus on learning!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a637db",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîç PART 3: Advanced MCP Features\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c605de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç PART 3: Advanced MCP Features\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç PART 3: Advanced MCP Features\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32c9f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Once you're comfortable with basics, explore:\n",
      "\n",
      "1. **Resources**: Provide access to files, databases, APIs\n",
      "2. **Prompts**: Create reusable prompt templates\n",
      "3. **Logging**: Add structured logging to your server\n",
      "4. **Authentication**: Secure your server with proper auth\n",
      "5. **Streaming**: Handle long-running operations\n",
      "6. **Error Handling**: Graceful error handling and recovery\n",
      "\n",
      "üöÄ **Real-World Examples**:\n",
      "- File system access (read/write files)\n",
      "- Database queries (SQL, NoSQL)\n",
      "- API integrations (weather, news, etc.)\n",
      "- System monitoring (CPU, memory, processes)\n",
      "- Custom business logic tools\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üîç Once you're comfortable with basics, explore:\n",
    "\n",
    "1. **Resources**: Provide access to files, databases, APIs\n",
    "2. **Prompts**: Create reusable prompt templates\n",
    "3. **Logging**: Add structured logging to your server\n",
    "4. **Authentication**: Secure your server with proper auth\n",
    "5. **Streaming**: Handle long-running operations\n",
    "6. **Error Handling**: Graceful error handling and recovery\n",
    "\n",
    "üöÄ **Real-World Examples**:\n",
    "- File system access (read/write files)\n",
    "- Database queries (SQL, NoSQL)\n",
    "- API integrations (weather, news, etc.)\n",
    "- System monitoring (CPU, memory, processes)\n",
    "- Custom business logic tools\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91754af8",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üìù PART 3 SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99530eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù PART 3 SUMMARY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù PART 3 SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df1e8fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ What We Accomplished:\n",
      "- Learned what MCP is and why it's useful\n",
      "- Created a simple MCP server with 3 tools\n",
      "- Understood how to connect from VS Code/Cursor\n",
      "- Explored advanced MCP capabilities\n",
      "\n",
      "üîë Key Concepts:\n",
      "- MCP Server: Provides tools and resources\n",
      "- MCP Client: Connects to servers (VS Code, etc.)\n",
      "- Tools: Functions the AI can call\n",
      "- Resources: Data the AI can access\n",
      "\n",
      "üöÄ Next Steps:\n",
      "- Run your server and test it\n",
      "- Connect it to VS Code/Cursor\n",
      "- Build more sophisticated tools\n",
      "- Explore the MCP ecosystem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚úÖ What We Accomplished:\n",
    "- Learned what MCP is and why it's useful\n",
    "- Created a simple MCP server with 3 tools\n",
    "- Understood how to connect from VS Code/Cursor\n",
    "- Explored advanced MCP capabilities\n",
    "\n",
    "üîë Key Concepts:\n",
    "- MCP Server: Provides tools and resources\n",
    "- MCP Client: Connects to servers (VS Code, etc.)\n",
    "- Tools: Functions the AI can call\n",
    "- Resources: Data the AI can access\n",
    "\n",
    "üöÄ Next Steps:\n",
    "- Run your server and test it\n",
    "- Connect it to VS Code/Cursor\n",
    "- Build more sophisticated tools\n",
    "- Explore the MCP ecosystem\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3496f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Part 3 Complete! You now have a working MCP server!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéâ Part 3 Complete! You now have a working MCP server!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7210f",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéâ WORKSHOP COMPLETE!\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e87f944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ WORKSHOP COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ WORKSHOP COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c928e4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåü **CONGRATULATIONS!** üåü\n",
      "\n",
      "You've completed the complete LLM workshop and now have:\n",
      "\n",
      "‚úÖ A working connection to your LLM endpoint  \n",
      "‚úÖ Experience with LangChain agents and tools  \n",
      "‚úÖ Your own MCP server with custom tools  \n",
      "‚úÖ Knowledge of how to integrate everything  \n",
      "\n",
      "**What You've Built:**\n",
      "1. **LLM Connection**: Direct API access to your endpoint\n",
      "2. **Custom Tools**: Calculator, weather, time tools\n",
      "3. **Smart Agents**: Agents that can reason and use tools\n",
      "4. **MCP Server**: Your own server that VS Code/Cursor can use\n",
      "\n",
      "**Next Steps:**\n",
      "- Customize your tools and agents\n",
      "- Build more sophisticated MCP servers\n",
      "- Integrate with real APIs and databases\n",
      "- Create your own AI-powered applications\n",
      "\n",
      "**Time to build something amazing! üöÄ**\n",
      "\n",
      "---\n",
      "\n",
      "**Workshop Files Created:**\n",
      "- `workshop_mcp_server.py` - Your working MCP server\n",
      "- All workshop code is now in Jupiter notebook format\n",
      "\n",
      "**Resources:**\n",
      "- Check README.md for detailed instructions\n",
      "- Use QUICK_START.md for quick reference\n",
      "- Explore the individual notebook parts for focused learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üåü **CONGRATULATIONS!** üåü\n",
    "\n",
    "You've completed the complete LLM workshop and now have:\n",
    "\n",
    "‚úÖ A working connection to your LLM endpoint  \n",
    "‚úÖ Experience with LangChain agents and tools  \n",
    "‚úÖ Your own MCP server with custom tools  \n",
    "‚úÖ Knowledge of how to integrate everything  \n",
    "\n",
    "**What You've Built:**\n",
    "1. **LLM Connection**: Direct API access to your endpoint\n",
    "2. **Custom Tools**: Calculator, weather, time tools\n",
    "3. **Smart Agents**: Agents that can reason and use tools\n",
    "4. **MCP Server**: Your own server that VS Code/Cursor can use\n",
    "\n",
    "**Next Steps:**\n",
    "- Customize your tools and agents\n",
    "- Build more sophisticated MCP servers\n",
    "- Integrate with real APIs and databases\n",
    "- Create your own AI-powered applications\n",
    "\n",
    "**Time to build something amazing! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "**Workshop Files Created:**\n",
    "- `workshop_mcp_server.py` - Your working MCP server\n",
    "- All workshop code is now in Jupiter notebook format\n",
    "\n",
    "**Resources:**\n",
    "- Check README.md for detailed instructions\n",
    "- Use QUICK_START.md for quick reference\n",
    "- Explore the individual notebook parts for focused learning\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cabff670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ **Happy coding and building!** üöÄ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ **Happy coding and building!** üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "llm_workshop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
