{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cada10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "🚀 LLM Workshop: Complete Master Edition\n",
    "Duration: 3 hours\n",
    "Level: Beginner\n",
    "Tools: VS Code, Cursor, or WindSurf\n",
    "\n",
    "IMPORTANT: Make sure your virtual environment is activated!\n",
    "If you haven't set up the environment yet, run: python3 workshop_setup.py\n",
    "\n",
    "This master file combines all three workshop parts:\n",
    "- Part 1: LLM Hello World\n",
    "- Part 2: LLM Agents  \n",
    "- Part 3: MCP Server Development\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9736d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf44d0",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🎯 WORKSHOP OVERVIEW\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38baae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 LLM WORKSHOP: COMPLETE MASTER EDITION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Duration: 3 hours | Level: Beginner\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🎯 What You'll Learn:\n",
    "\n",
    "Part 1: LLM Hello World (45 min)\n",
    "- Connect to your LLM endpoint\n",
    "- Check available models  \n",
    "- Generate text with simple prompts\n",
    "- Have conversations with the LLM\n",
    "- Experiment with temperature and tokens\n",
    "\n",
    "Part 2: LLM Agents (45 min)\n",
    "- Create custom tools (calculator, weather, time)\n",
    "- Build a tool-calling agent with LangChain\n",
    "- Create a conversational agent with memory\n",
    "- See how agents reason through complex tasks\n",
    "\n",
    "Part 3: MCP Server (45 min)\n",
    "- Learn what MCP is and why it's useful\n",
    "- Create a simple MCP server with 3 tools\n",
    "- Understand how to connect from VS Code/Cursor\n",
    "- Explore advanced MCP capabilities\n",
    "\n",
    "Let's get started! 🎉\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802e550",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🔧 PART 1: SETUP AND CONFIGURATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55670595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔧 PART 1: Setup and Configuration\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ddae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = os.getenv(\"BASE_URL\", \"https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod/v1\")\n",
    "API_KEY = os.getenv(\"API_KEY\", \"ALI-CLASS-2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Configuration loaded!\")\n",
    "print(f\"🌐 Base URL: {BASE_URL}\")\n",
    "print(f\"🔑 API Key: {API_KEY[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c2bfc",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🧪 PART 1: TEST 1 - Check Available Models\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44424a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🧪 PART 1: Test 1 - Check Available Models\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112bae5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_models():\n",
    "    \"\"\"Check what models are available on your endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/v1/models\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(\"✅ Successfully connected to your endpoint!\")\n",
    "            print(\"📋 Available models:\")\n",
    "            for model in models.get('data', []):\n",
    "                print(f\"   - {model.get('id', 'Unknown')}\")\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"❌ Error: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Connection error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection\n",
    "models = check_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e4f9c",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🌟 PART 1: LLM Hello World - Basic Text Generation\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605950dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🌟 PART 1: LLM Hello World - Basic Text Generation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f64c4b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Generate text using your LLM endpoint\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple greeting\n",
    "print(\"🤖 Test 1: Simple Greeting\")\n",
    "prompt1 = \"Hello! Can you give me a friendly greeting in 2 sentences?\"\n",
    "response1 = generate_text(prompt1)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Response: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdfca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Creative writing\n",
    "print(\"\\n🤖 Test 2: Creative Writing\")\n",
    "prompt2 = \"Write a short, fun story about a robot learning to cook (max 3 sentences)\"\n",
    "response2 = generate_text(prompt2)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Code explanation\n",
    "print(\"\\n🤖 Test 3: Code Explanation\")\n",
    "prompt3 = \"Explain what a Python function is in simple terms (max 2 sentences)\"\n",
    "response3 = generate_text(prompt3)\n",
    "print(f\"Prompt: {prompt3}\")\n",
    "print(f\"Response: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b4ffc",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🔄 PART 1: LLM Hello World - Chat Completion\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827e1f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔄 PART 1: LLM Hello World - Chat Completion\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2457d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def chat_completion(messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Have a conversation with your LLM\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bda528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "print(\"💬 Starting a conversation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm learning about LLMs. Can you help me?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First response\n",
    "response = chat_completion(conversation)\n",
    "print(f\"🤖 Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to conversation and continue\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"What's the most exciting thing about LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chat_completion(conversation)\n",
    "print(f\"🤖 Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b8400",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🎯 PART 1: Hands-On Exercise\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e896bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎯 PART 1: Hands-On Exercise\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa99d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🎯 Your Turn! Try these exercises:\n",
    "\n",
    "1. **Custom Prompt**: Create your own prompt and see what the LLM generates\n",
    "   - Try asking about your favorite hobby\n",
    "   - Ask for a recipe or travel tip\n",
    "   - Request a poem or joke\n",
    "\n",
    "2. **Temperature Experiment**: Change the temperature value (0.1 to 1.0)\n",
    "   - Lower = more focused/consistent\n",
    "   - Higher = more creative/varied\n",
    "\n",
    "3. **Token Limit**: Experiment with max_tokens\n",
    "   - Try 50, 100, 200 tokens\n",
    "   - See how it affects response length\n",
    "\n",
    "4. **Model Comparison**: If you have multiple models, compare their outputs\n",
    "\n",
    "💡 Tips:\n",
    "- Keep prompts clear and specific\n",
    "- Start with simple requests\n",
    "- Don't worry about perfect responses - this is learning!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a8b45",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "📝 PART 1 SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83613fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📝 PART 1 SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "✅ What We Accomplished:\n",
    "- Connected to your LLM endpoint\n",
    "- Checked available models\n",
    "- Generated text with simple prompts\n",
    "- Had a conversation with the LLM\n",
    "- Learned about temperature and tokens\n",
    "\n",
    "🔑 Key Concepts:\n",
    "- API endpoints and authentication\n",
    "- Text generation vs chat completion\n",
    "- Prompt engineering basics\n",
    "- Model parameters (temperature, max_tokens)\n",
    "\n",
    "🚀 Next Up: LLM Agents with LangChain!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎉 Part 1 Complete! Ready for Part 2: LLM Agents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1a91c",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🤖 PART 2: LLM AGENTS WITH LANGCHAIN\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99492eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🤖 PART 2: LLM Agents with LangChain\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d79979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🛠️ Creating Custom Tools for Our Agent\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b22a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import LangChain components\n",
    "try:\n",
    "    from langchain.agents import initialize_agent, AgentType, Tool\n",
    "    from langchain.tools import BaseTool\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    \n",
    "    print(\"✅ LangChain imports successful!\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 🛠️ CUSTOM TOOLS FOR OUR AGENT\n",
    "    # ============================================================================\n",
    "    \n",
    "    class CalculatorTool(BaseTool):\n",
    "        \"\"\"Simple calculator tool for the agent\"\"\"\n",
    "        name: str = \"calculator\"\n",
    "        description: str = \"Useful for doing math calculations. Input should be a mathematical expression like '2 + 2' or '10 * 5'\"\n",
    "        \n",
    "        def _run(self, query: str) -> str:\n",
    "            \"\"\"Execute the calculation\"\"\"\n",
    "            try:\n",
    "                # Simple and safe evaluation - only basic math operations\n",
    "                allowed_chars = set('0123456789+-*/.() ')\n",
    "                if not all(c in allowed_chars for c in query):\n",
    "                    return \"Error: Only basic math operations (+, -, *, /, .) and numbers are allowed\"\n",
    "                \n",
    "                result = eval(query)\n",
    "                return f\"Result: {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error calculating {query}: {str(e)}\"\n",
    "\n",
    "    class WeatherTool(BaseTool):\n",
    "        \"\"\"Mock weather tool for demonstration\"\"\"\n",
    "        name: str = \"weather\"\n",
    "        description: str = \"Get weather information for a city. Input should be a city name like 'London' or 'New York'\"\n",
    "        \n",
    "        def _run(self, city: str) -> str:\n",
    "            \"\"\"Mock weather response\"\"\"\n",
    "            # In a real scenario, this would call a weather API\n",
    "            weather_data = {\n",
    "                \"London\": \"🌧️ Cloudy with rain, 15°C\",\n",
    "                \"New York\": \"☀️ Sunny, 22°C\", \n",
    "                \"Tokyo\": \"⛅ Partly cloudy, 18°C\",\n",
    "                \"Sydney\": \"🌤️ Mostly sunny, 25°C\"\n",
    "            }\n",
    "            \n",
    "            if city in weather_data:\n",
    "                return f\"Weather in {city}: {weather_data[city]}\"\n",
    "            else:\n",
    "                return f\"Weather for {city}: ☀️ Nice weather, 20°C (mock data)\"\n",
    "\n",
    "    class TimeTool(BaseTool):\n",
    "        \"\"\"Simple time tool\"\"\"\n",
    "        name: str = \"get_time\"\n",
    "        description: str = \"Get the current time. No input needed.\"\n",
    "        \n",
    "        def _run(self, query: str = \"\") -> str:\n",
    "            \"\"\"Get current time\"\"\"\n",
    "            from datetime import datetime\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            return f\"Current time: {current_time}\"\n",
    "\n",
    "    # Create our tools\n",
    "    tools = [\n",
    "        CalculatorTool(),\n",
    "        WeatherTool(),\n",
    "        TimeTool()\n",
    "    ]\n",
    "\n",
    "    print(\"✅ Created 3 custom tools:\")\n",
    "    for tool in tools:\n",
    "        print(f\"   - {tool.name}: {tool.description}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 🤖 AGENT 1: TOOL-CALLING AGENT\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🤖 PART 2: Agent 1 - Tool-Calling Agent\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configure the LLM (using your endpoint)\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"BASE_URL\", \"https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod/v1\"),\n",
    "        api_key=os.getenv(\"API_KEY\", \"ALI-CLASS-2025\"),\n",
    "        model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Initialize the agent\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ Agent initialized with tools!\")\n",
    "    print(\"🤖 Testing tool calling capabilities...\")\n",
    "\n",
    "    # Test 1: Math calculation\n",
    "    print(\"\\n🧮 Test 1: Math Calculation\")\n",
    "    try:\n",
    "        response1 = agent.run(\"What is 15 * 8 + 3?\")\n",
    "        print(f\"Agent Response: {response1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Test 2: Weather information\n",
    "    print(\"\\n🌤️ Test 2: Weather Information\")\n",
    "    try:\n",
    "        response2 = agent.run(\"What's the weather like in London?\")\n",
    "        print(f\"Agent Response: {response2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Test 3: Time request\n",
    "    print(\"\\n⏰ Test 3: Time Request\")\n",
    "    try:\n",
    "        response3 = agent.run(\"What time is it right now?\")\n",
    "        print(f\"Agent Response: {response3}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 🤖 AGENT 2: CONVERSATIONAL AGENT WITH MEMORY\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🤖 PART 2: Agent 2 - Conversational Agent with Memory\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create a memory component\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    # Create a conversational agent\n",
    "    conversational_agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ Conversational agent with memory initialized!\")\n",
    "    print(\"💬 Testing conversation flow...\")\n",
    "\n",
    "    # Start a conversation\n",
    "    print(\"\\n💬 Starting conversation...\")\n",
    "\n",
    "    try:\n",
    "        # First message\n",
    "        response1 = conversational_agent.run(\"Hi! I'm planning a trip. Can you help me?\")\n",
    "        print(f\"🤖 Agent: {response1}\")\n",
    "        \n",
    "        # Second message (agent should remember context)\n",
    "        response2 = conversational_agent.run(\"What's the weather like in Tokyo?\")\n",
    "        print(f\"🤖 Agent: {response2}\")\n",
    "        \n",
    "        # Third message (testing memory)\n",
    "        response3 = conversational_agent.run(\"And what about the weather in London?\")\n",
    "        print(f\"🤖 Agent: {response3}\")\n",
    "        \n",
    "        # Fourth message (testing tool combination)\n",
    "        response4 = conversational_agent.run(\"If I have 500 dollars and spend 120 on flights, how much do I have left?\")\n",
    "        print(f\"🤖 Agent: {response4}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in conversation: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 🎯 PART 2: Hands-On Exercise\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎯 PART 2: Hands-On Exercise\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "🎯 Your Turn! Try these exercises:\n",
    "\n",
    "1. **Tool Testing**: Ask the agent to use different tools\n",
    "   - \"Calculate 25 * 4 / 2\"\n",
    "   - \"What's the weather in Sydney?\"\n",
    "   - \"What time is it?\"\n",
    "\n",
    "2. **Multi-Step Tasks**: Give the agent complex requests\n",
    "   - \"If I have $1000 and spend $300 on a hotel, $150 on food, how much do I have left?\"\n",
    "   - \"What's the weather in New York and what time is it there?\"\n",
    "\n",
    "3. **Conversation Flow**: Have a natural conversation\n",
    "   - Ask about planning a weekend trip\n",
    "   - Request calculations for a budget\n",
    "   - Ask about weather in different cities\n",
    "\n",
    "4. **Custom Tool**: Try creating your own simple tool\n",
    "   - A tool that returns random facts\n",
    "   - A tool that converts units\n",
    "   - A tool that gives motivational quotes\n",
    "\n",
    "💡 Tips:\n",
    "- Be specific in your requests\n",
    "- Let the agent use multiple tools when needed\n",
    "- Watch how it reasons through complex tasks\n",
    "- Don't worry if it makes mistakes - this is learning!\n",
    "\"\"\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 🔍 PART 2: Agent Reasoning Demonstration\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🔍 PART 2: Agent Reasoning Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "🔍 Let's see how the agent thinks through a complex task:\n",
    "\n",
    "Task: \"I have $500 and want to plan a weekend trip. \n",
    "I need to spend $200 on accommodation, $100 on food, \n",
    "and I want to know how much I'll have left for activities.\"\n",
    "\n",
    "The agent should:\n",
    "1. Use the calculator to subtract expenses: $500 - $200 - $100\n",
    "2. Give you the remaining budget\n",
    "3. Suggest what you could do with the remaining money\n",
    "\n",
    "This shows the agent's ability to:\n",
    "- Break down complex requests\n",
    "- Use multiple tools\n",
    "- Provide helpful, contextual responses\n",
    "\"\"\")\n",
    "\n",
    "    # Test the complex task\n",
    "    print(\"\\n🧪 Testing Complex Task...\")\n",
    "    try:\n",
    "        complex_response = conversational_agent.run(\n",
    "            \"I have $500 and want to plan a weekend trip. \"\n",
    "            \"I need to spend $200 on accommodation, $100 on food, \"\n",
    "            \"and I want to know how much I'll have left for activities.\"\n",
    "        )\n",
    "        print(f\"🤖 Agent Response: {complex_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 📝 PART 2 SUMMARY\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📝 PART 2 SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "✅ What We Accomplished:\n",
    "- Created custom tools (calculator, weather, time)\n",
    "- Built a tool-calling agent with LangChain\n",
    "- Created a conversational agent with memory\n",
    "- Saw how agents reason through complex tasks\n",
    "- Tested multi-step problem solving\n",
    "\n",
    "🔑 Key Concepts:\n",
    "- Tools: Functions agents can call\n",
    "- Agents: LLMs that can use tools\n",
    "- Memory: How agents remember conversations\n",
    "- Reasoning: How agents break down complex tasks\n",
    "\n",
    "🚀 Next Up: Building Your Own MCP Server!\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\\n🎉 Part 2 Complete! Ready for Part 3: MCP Server Development?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6313022",
   "metadata": {},
   "outputs": [],
   "source": [
    "except ImportError as e:\n",
    "    print(f\"⚠️  LangChain not available: {e}\")\n",
    "    print(\"💡 Install with: pip install langchain langchain-openai\")\n",
    "    print(\"🚀 Continuing to Part 3...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35f6f7",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🔌 PART 3: MCP SERVER DEVELOPMENT\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9daa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔌 PART 3: MCP Server Development\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📚 Understanding MCP (Model Context Protocol)\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🔍 MCP (Model Context Protocol) is a way for AI models to:\n",
    "- Connect to external tools and resources\n",
    "- Access real-time information\n",
    "- Interact with your local system\n",
    "- Extend their capabilities beyond just text\n",
    "\n",
    "💡 Think of it as giving your AI a \"remote control\" to your computer!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f830af",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🛠️ PART 3: Installing MCP Requirements\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56f027",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🛠️ PART 3: Installing MCP Requirements\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996c273",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def install_mcp_requirements():\n",
    "    \"\"\"Install required packages for MCP development\"\"\"\n",
    "    packages = [\n",
    "        \"mcp\"\n",
    "    ]\n",
    "    \n",
    "    print(\"📦 Installing MCP packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"✅ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"❌ Failed to install {package}\")\n",
    "            print(\"💡 You may need to install manually: pip install mcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511b08e",
   "metadata": {},
   "source": [
    "Uncomment the line below to install packages\n",
    "install_mcp_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💡 If you haven't installed MCP packages yet, run:\")\n",
    "print(\"   pip install mcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514a609",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🚀 PART 3: Creating Your First MCP Server\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🚀 PART 3: Creating Your First MCP Server\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MCP server\n",
    "mcp_server_code = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "🎯 Simple MCP Server - Hello World\n",
    "This server provides basic tools for demonstration\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "class SimpleMCPServer:\n",
    "    \"\"\"Simple MCP Server implementation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools = {\n",
    "            \"hello_world\": {\n",
    "                \"description\": \"A simple hello world tool that greets the user\",\n",
    "                \"inputSchema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Your name (optional)\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"get_current_time\": {\n",
    "                \"description\": \"Get the current date and time\",\n",
    "                \"inputSchema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {}\n",
    "                }\n",
    "            },\n",
    "            \"calculate\": {\n",
    "                \"description\": \"Perform basic mathematical calculations\",\n",
    "                \"inputSchema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Mathematical expression to evaluate\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    async def handle_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle incoming MCP requests\"\"\"\n",
    "        \n",
    "        method = request.get(\"method\")\n",
    "        request_id = request.get(\"id\")\n",
    "        \n",
    "        if method == \"initialize\":\n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"result\": {\n",
    "                    \"protocolVersion\": \"2024-11-05\",\n",
    "                    \"capabilities\": {\n",
    "                        \"tools\": {}\n",
    "                    },\n",
    "                    \"serverInfo\": {\n",
    "                        \"name\": \"workshop-mcp-server\",\n",
    "                        \"version\": \"1.0.0\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        elif method == \"tools/list\":\n",
    "            tools_list = []\n",
    "            for name, tool_info in self.tools.items():\n",
    "                tools_list.append({\n",
    "                    \"name\": name,\n",
    "                    \"description\": tool_info[\"description\"],\n",
    "                    \"inputSchema\": tool_info[\"inputSchema\"]\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"result\": {\n",
    "                    \"tools\": tools_list\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        elif method == \"tools/call\":\n",
    "            tool_name = request.get(\"params\", {}).get(\"name\")\n",
    "            arguments = request.get(\"params\", {}).get(\"arguments\", {})\n",
    "            \n",
    "            result = await self.execute_tool(tool_name, arguments)\n",
    "            \n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"result\": {\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": result\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"error\": {\n",
    "                    \"code\": -32601,\n",
    "                    \"message\": f\"Method not found: {method}\"\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    async def execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute the requested tool\"\"\"\n",
    "        \n",
    "        if tool_name == \"hello_world\":\n",
    "            name = arguments.get(\"name\", \"there\")\n",
    "            return f\"Hello {name}! 👋 Welcome to your first MCP server!\"\n",
    "        \n",
    "        elif tool_name == \"get_current_time\":\n",
    "            now = datetime.now()\n",
    "            return f\"🕐 Current time: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        \n",
    "        elif tool_name == \"calculate\":\n",
    "            expression = arguments.get(\"expression\", \"\")\n",
    "            try:\n",
    "                # Simple and safe evaluation for basic math\n",
    "                allowed_chars = set('0123456789+-*/.() ')\n",
    "                if not all(c in allowed_chars for c in expression):\n",
    "                    return \"❌ Error: Only basic mathematical expressions are allowed\"\n",
    "                \n",
    "                result = eval(expression)\n",
    "                return f\"🧮 {expression} = {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"❌ Error calculating '{expression}': {str(e)}\"\n",
    "        \n",
    "        else:\n",
    "            return f\"❌ Unknown tool: {tool_name}\"\n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"Run the MCP server\"\"\"\n",
    "        print(\"🚀 Starting Simple MCP Server...\", file=sys.stderr)\n",
    "        print(\"📋 Available tools:\", file=sys.stderr)\n",
    "        for name, tool_info in self.tools.items():\n",
    "            print(f\"   - {name}: {tool_info['description']}\", file=sys.stderr)\n",
    "        print(\"🔌 Server ready for connections!\", file=sys.stderr)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Read request from stdin\n",
    "                line = await asyncio.get_event_loop().run_in_executor(None, sys.stdin.readline)\n",
    "                if not line:\n",
    "                    break\n",
    "                \n",
    "                request = json.loads(line.strip())\n",
    "                response = await self.handle_request(request)\n",
    "                \n",
    "                # Send response to stdout\n",
    "                print(json.dumps(response))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\", file=sys.stderr)\n",
    "                break\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    server = SimpleMCPServer()\n",
    "    await server.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ef8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MCP server code\n",
    "with open(\"workshop_mcp_server.py\", \"w\") as f:\n",
    "    f.write(mcp_server_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b328f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Created workshop_mcp_server.py\")\n",
    "print(\"📋 This server provides 3 simple tools:\")\n",
    "print(\"   - hello_world: Greet the user\")\n",
    "print(\"   - get_current_time: Get current time\")\n",
    "print(\"   - calculate_simple: Do math calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65925438",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🧪 PART 3: Testing Your MCP Server\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🧪 PART 3: Testing Your MCP Server\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🧪 To test your MCP server:\n",
    "\n",
    "1. **Start the server** (in a new terminal):\n",
    "   python workshop_mcp_server.py\n",
    "\n",
    "2. **Test with curl** (in another terminal):\n",
    "   curl -X POST http://localhost:8000/call_tool \\\\\n",
    "        -H \"Content-Type: application/json\" \\\\\n",
    "        -d '{\"name\": \"hello_world\", \"arguments\": {\"name\": \"Alice\"}}'\n",
    "\n",
    "3. **Or use the MCP client**:\n",
    "   python -c \"\n",
    "   from mcp.client.stdio import stdio_client\n",
    "   import asyncio\n",
    "   \n",
    "   async def test():\n",
    "       async with stdio_client(['python', 'workshop_mcp_server.py']) as client:\n",
    "           tools = await client.list_tools()\n",
    "           print('Available tools:', tools)\n",
    "   \n",
    "   asyncio.run(test())\n",
    "   \"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f538a9",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🔌 PART 3: Connecting from VS Code/Cursor\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔌 PART 3: Connecting from VS Code/Cursor\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4acdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🔌 To connect your MCP server to VS Code/Cursor:\n",
    "\n",
    "1. **Install MCP Extension**:\n",
    "   - VS Code: Search for \"MCP\" in extensions\n",
    "   - Cursor: Should have MCP support built-in\n",
    "\n",
    "2. **Configure MCP Client**:\n",
    "   Create ~/.config/mcp/clients.json:\n",
    "   {\n",
    "     \"workshop-server\": {\n",
    "       \"command\": \"python\",\n",
    "       \"args\": [\"/path/to/your/workshop_mcp_server.py\"],\n",
    "       \"env\": {}\n",
    "     }\n",
    "   }\n",
    "\n",
    "3. **Restart VS Code/Cursor**:\n",
    "   - The MCP server should now be available\n",
    "   - You can ask the AI to use your custom tools!\n",
    "\n",
    "4. **Test the Connection**:\n",
    "   Ask: \"What time is it?\" or \"Calculate 15 * 3\"\n",
    "   The AI should use your MCP server tools!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b29b2",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🎯 PART 3: Hands-On Exercise\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎯 PART 3: Hands-On Exercise\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746398e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🎯 Your Turn! Try these exercises:\n",
    "\n",
    "1. **Run Your Server**:\n",
    "   - Start the MCP server in a terminal\n",
    "   - Test it with simple requests\n",
    "   - Make sure it responds correctly\n",
    "\n",
    "2. **Add a New Tool**:\n",
    "   - Create a tool that returns random facts\n",
    "   - Add a tool that converts temperatures\n",
    "   - Build a tool that gives motivational quotes\n",
    "\n",
    "3. **Connect to VS Code/Cursor**:\n",
    "   - Set up the MCP client configuration\n",
    "   - Restart your editor\n",
    "   - Ask the AI to use your tools\n",
    "\n",
    "4. **Debug and Improve**:\n",
    "   - Add error handling to your tools\n",
    "   - Improve the tool descriptions\n",
    "   - Add input validation\n",
    "\n",
    "💡 Tips:\n",
    "- Start simple and build up\n",
    "- Test each tool individually\n",
    "- Check the MCP documentation for more features\n",
    "- Don't worry about perfection - focus on learning!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090de2b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🔍 PART 3: Advanced MCP Features\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔍 PART 3: Advanced MCP Features\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🔍 Once you're comfortable with basics, explore:\n",
    "\n",
    "1. **Resources**: Provide access to files, databases, APIs\n",
    "2. **Prompts**: Create reusable prompt templates\n",
    "3. **Logging**: Add structured logging to your server\n",
    "4. **Authentication**: Secure your server with proper auth\n",
    "5. **Streaming**: Handle long-running operations\n",
    "6. **Error Handling**: Graceful error handling and recovery\n",
    "\n",
    "🚀 **Real-World Examples**:\n",
    "- File system access (read/write files)\n",
    "- Database queries (SQL, NoSQL)\n",
    "- API integrations (weather, news, etc.)\n",
    "- System monitoring (CPU, memory, processes)\n",
    "- Custom business logic tools\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2e3df",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "📝 PART 3 SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fe680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📝 PART 3 SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "✅ What We Accomplished:\n",
    "- Learned what MCP is and why it's useful\n",
    "- Created a simple MCP server with 3 tools\n",
    "- Understood how to connect from VS Code/Cursor\n",
    "- Explored advanced MCP capabilities\n",
    "\n",
    "🔑 Key Concepts:\n",
    "- MCP Server: Provides tools and resources\n",
    "- MCP Client: Connects to servers (VS Code, etc.)\n",
    "- Tools: Functions the AI can call\n",
    "- Resources: Data the AI can access\n",
    "\n",
    "🚀 Next Steps:\n",
    "- Run your server and test it\n",
    "- Connect it to VS Code/Cursor\n",
    "- Build more sophisticated tools\n",
    "- Explore the MCP ecosystem\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f90d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎉 Part 3 Complete! You now have a working MCP server!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596e738",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🎉 WORKSHOP COMPLETE!\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 WORKSHOP COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "🌟 **CONGRATULATIONS!** 🌟\n",
    "\n",
    "You've completed the complete LLM workshop and now have:\n",
    "\n",
    "✅ A working connection to your LLM endpoint  \n",
    "✅ Experience with LangChain agents and tools  \n",
    "✅ Your own MCP server with custom tools  \n",
    "✅ Knowledge of how to integrate everything  \n",
    "\n",
    "**What You've Built:**\n",
    "1. **LLM Connection**: Direct API access to your endpoint\n",
    "2. **Custom Tools**: Calculator, weather, time tools\n",
    "3. **Smart Agents**: Agents that can reason and use tools\n",
    "4. **MCP Server**: Your own server that VS Code/Cursor can use\n",
    "\n",
    "**Next Steps:**\n",
    "- Customize your tools and agents\n",
    "- Build more sophisticated MCP servers\n",
    "- Integrate with real APIs and databases\n",
    "- Create your own AI-powered applications\n",
    "\n",
    "**Time to build something amazing! 🚀**\n",
    "\n",
    "---\n",
    "\n",
    "**Workshop Files Created:**\n",
    "- `workshop_mcp_server.py` - Your working MCP server\n",
    "- All workshop code is now in Jupiter notebook format\n",
    "\n",
    "**Resources:**\n",
    "- Check README.md for detailed instructions\n",
    "- Use QUICK_START.md for quick reference\n",
    "- Explore the individual notebook parts for focused learning\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎯 **Happy coding and building!** 🚀\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
