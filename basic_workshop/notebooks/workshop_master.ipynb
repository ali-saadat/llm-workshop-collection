{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cada10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üöÄ LLM Workshop: Complete Master Edition\n",
    "Duration: 3 hours\n",
    "Level: Beginner\n",
    "Tools: VS Code, Cursor, or WindSurf\n",
    "\n",
    "IMPORTANT: Make sure your virtual environment is activated!\n",
    "If you haven't set up the environment yet, run: python3 workshop_setup.py\n",
    "\n",
    "This master file combines all three workshop parts:\n",
    "- Part 1: LLM Hello World\n",
    "- Part 2: LLM Agents  \n",
    "- Part 3: MCP Server Development\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9736d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf44d0",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéØ WORKSHOP OVERVIEW\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38baae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ LLM WORKSHOP: COMPLETE MASTER EDITION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Duration: 3 hours | Level: Beginner\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üéØ What You'll Learn:\n",
    "\n",
    "Part 1: LLM Hello World (45 min)\n",
    "- Connect to your LLM endpoint\n",
    "- Check available models  \n",
    "- Generate text with simple prompts\n",
    "- Have conversations with the LLM\n",
    "- Experiment with temperature and tokens\n",
    "\n",
    "Part 2: LLM Agents (45 min)\n",
    "- Create custom tools (calculator, weather, time)\n",
    "- Build a tool-calling agent with LangChain\n",
    "- Create a conversational agent with memory\n",
    "- See how agents reason through complex tasks\n",
    "\n",
    "Part 3: MCP Server (45 min)\n",
    "- Learn what MCP is and why it's useful\n",
    "- Create a simple MCP server with 3 tools\n",
    "- Understand how to connect from VS Code/Cursor\n",
    "- Explore advanced MCP capabilities\n",
    "\n",
    "Let's get started! üéâ\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802e550",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîß PART 1: SETUP AND CONFIGURATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55670595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîß PART 1: Setup and Configuration\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ddae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = os.getenv(\"BASE_URL\", \"https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod/v1\")\n",
    "API_KEY = os.getenv(\"API_KEY\", \"ALI-CLASS-2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"üåê Base URL: {BASE_URL}\")\n",
    "print(f\"üîë API Key: {API_KEY[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c2bfc",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üß™ PART 1: TEST 1 - Check Available Models\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44424a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß™ PART 1: Test 1 - Check Available Models\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112bae5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_models():\n",
    "    \"\"\"Check what models are available on your endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/v1/models\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(\"‚úÖ Successfully connected to your endpoint!\")\n",
    "            print(\"üìã Available models:\")\n",
    "            for model in models.get('data', []):\n",
    "                print(f\"   - {model.get('id', 'Unknown')}\")\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection\n",
    "models = check_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e4f9c",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üåü PART 1: LLM Hello World - Basic Text Generation\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605950dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üåü PART 1: LLM Hello World - Basic Text Generation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f64c4b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Generate text using your LLM endpoint\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple greeting\n",
    "print(\"ü§ñ Test 1: Simple Greeting\")\n",
    "prompt1 = \"Hello! Can you give me a friendly greeting in 2 sentences?\"\n",
    "response1 = generate_text(prompt1)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Response: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdfca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Creative writing\n",
    "print(\"\\nü§ñ Test 2: Creative Writing\")\n",
    "prompt2 = \"Write a short, fun story about a robot learning to cook (max 3 sentences)\"\n",
    "response2 = generate_text(prompt2)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Code explanation\n",
    "print(\"\\nü§ñ Test 3: Code Explanation\")\n",
    "prompt3 = \"Explain what a Python function is in simple terms (max 2 sentences)\"\n",
    "response3 = generate_text(prompt3)\n",
    "print(f\"Prompt: {prompt3}\")\n",
    "print(f\"Response: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b4ffc",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîÑ PART 1: LLM Hello World - Chat Completion\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827e1f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ PART 1: LLM Hello World - Chat Completion\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2457d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def chat_completion(messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Have a conversation with your LLM\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bda528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "print(\"üí¨ Starting a conversation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm learning about LLMs. Can you help me?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First response\n",
    "response = chat_completion(conversation)\n",
    "print(f\"ü§ñ Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to conversation and continue\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"What's the most exciting thing about LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chat_completion(conversation)\n",
    "print(f\"ü§ñ Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b8400",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéØ PART 1: Hands-On Exercise\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e896bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ PART 1: Hands-On Exercise\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa99d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üéØ Your Turn! Try these exercises:\n",
    "\n",
    "1. **Custom Prompt**: Create your own prompt and see what the LLM generates\n",
    "   - Try asking about your favorite hobby\n",
    "   - Ask for a recipe or travel tip\n",
    "   - Request a poem or joke\n",
    "\n",
    "2. **Temperature Experiment**: Change the temperature value (0.1 to 1.0)\n",
    "   - Lower = more focused/consistent\n",
    "   - Higher = more creative/varied\n",
    "\n",
    "3. **Token Limit**: Experiment with max_tokens\n",
    "   - Try 50, 100, 200 tokens\n",
    "   - See how it affects response length\n",
    "\n",
    "4. **Model Comparison**: If you have multiple models, compare their outputs\n",
    "\n",
    "üí° Tips:\n",
    "- Keep prompts clear and specific\n",
    "- Start with simple requests\n",
    "- Don't worry about perfect responses - this is learning!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a8b45",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üìù PART 1 SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83613fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù PART 1 SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚úÖ What We Accomplished:\n",
    "- Connected to your LLM endpoint\n",
    "- Checked available models\n",
    "- Generated text with simple prompts\n",
    "- Had a conversation with the LLM\n",
    "- Learned about temperature and tokens\n",
    "\n",
    "üîë Key Concepts:\n",
    "- API endpoints and authentication\n",
    "- Text generation vs chat completion\n",
    "- Prompt engineering basics\n",
    "- Model parameters (temperature, max_tokens)\n",
    "\n",
    "üöÄ Next Up: LLM Agents with LangChain!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéâ Part 1 Complete! Ready for Part 2: LLM Agents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1a91c",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "ü§ñ PART 2: LLM AGENTS WITH LANGCHAIN\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99492eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ü§ñ PART 2: LLM Agents with LangChain\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d79979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõ†Ô∏è Creating Custom Tools for Our Agent\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b22a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import LangChain components\n",
    "try:\n",
    "    from langchain.agents import initialize_agent, AgentType, Tool\n",
    "    from langchain.tools import BaseTool\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    \n",
    "    print(\"‚úÖ LangChain imports successful!\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # üõ†Ô∏è CUSTOM TOOLS FOR OUR AGENT\n",
    "    # ============================================================================\n",
    "    \n",
    "    class CalculatorTool(BaseTool):\n",
    "        \"\"\"Simple calculator tool for the agent\"\"\"\n",
    "        name: str = \"calculator\"\n",
    "        description: str = \"Useful for doing math calculations. Input should be a mathematical expression like '2 + 2' or '10 * 5'\"\n",
    "        \n",
    "        def _run(self, query: str) -> str:\n",
    "            \"\"\"Execute the calculation\"\"\"\n",
    "            try:\n",
    "                # Simple and safe evaluation - only basic math operations\n",
    "                allowed_chars = set('0123456789+-*/.() ')\n",
    "                if not all(c in allowed_chars for c in query):\n",
    "                    return \"Error: Only basic math operations (+, -, *, /, .) and numbers are allowed\"\n",
    "                \n",
    "                result = eval(query)\n",
    "                return f\"Result: {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error calculating {query}: {str(e)}\"\n",
    "\n",
    "    class WeatherTool(BaseTool):\n",
    "        \"\"\"Mock weather tool for demonstration\"\"\"\n",
    "        name: str = \"weather\"\n",
    "        description: str = \"Get weather information for a city. Input should be a city name like 'London' or 'New York'\"\n",
    "        \n",
    "        def _run(self, city: str) -> str:\n",
    "            \"\"\"Mock weather response\"\"\"\n",
    "            # In a real scenario, this would call a weather API\n",
    "            weather_data = {\n",
    "                \"London\": \"üåßÔ∏è Cloudy with rain, 15¬∞C\",\n",
    "                \"New York\": \"‚òÄÔ∏è Sunny, 22¬∞C\", \n",
    "                \"Tokyo\": \"‚õÖ Partly cloudy, 18¬∞C\",\n",
    "                \"Sydney\": \"üå§Ô∏è Mostly sunny, 25¬∞C\"\n",
    "            }\n",
    "            \n",
    "            if city in weather_data:\n",
    "                return f\"Weather in {city}: {weather_data[city]}\"\n",
    "            else:\n",
    "                return f\"Weather for {city}: ‚òÄÔ∏è Nice weather, 20¬∞C (mock data)\"\n",
    "\n",
    "    class TimeTool(BaseTool):\n",
    "        \"\"\"Simple time tool\"\"\"\n",
    "        name: str = \"get_time\"\n",
    "        description: str = \"Get the current time. No input needed.\"\n",
    "        \n",
    "        def _run(self, query: str = \"\") -> str:\n",
    "            \"\"\"Get current time\"\"\"\n",
    "            from datetime import datetime\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            return f\"Current time: {current_time}\"\n",
    "\n",
    "    # Create our tools\n",
    "    tools = [\n",
    "        CalculatorTool(),\n",
    "        WeatherTool(),\n",
    "        TimeTool()\n",
    "    ]\n",
    "\n",
    "    print(\"‚úÖ Created 3 custom tools:\")\n",
    "    for tool in tools:\n",
    "        print(f\"   - {tool.name}: {tool.description}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # ü§ñ AGENT 1: TOOL-CALLING AGENT\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ü§ñ PART 2: Agent 1 - Tool-Calling Agent\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configure the LLM (using your endpoint)\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"BASE_URL\", \"https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod/v1\"),\n",
    "        api_key=os.getenv(\"API_KEY\", \"ALI-CLASS-2025\"),\n",
    "        model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Initialize the agent\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Agent initialized with tools!\")\n",
    "    print(\"ü§ñ Testing tool calling capabilities...\")\n",
    "\n",
    "    # Test 1: Math calculation\n",
    "    print(\"\\nüßÆ Test 1: Math Calculation\")\n",
    "    try:\n",
    "        response1 = agent.run(\"What is 15 * 8 + 3?\")\n",
    "        print(f\"Agent Response: {response1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Test 2: Weather information\n",
    "    print(\"\\nüå§Ô∏è Test 2: Weather Information\")\n",
    "    try:\n",
    "        response2 = agent.run(\"What's the weather like in London?\")\n",
    "        print(f\"Agent Response: {response2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Test 3: Time request\n",
    "    print(\"\\n‚è∞ Test 3: Time Request\")\n",
    "    try:\n",
    "        response3 = agent.run(\"What time is it right now?\")\n",
    "        print(f\"Agent Response: {response3}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # ü§ñ AGENT 2: CONVERSATIONAL AGENT WITH MEMORY\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ü§ñ PART 2: Agent 2 - Conversational Agent with Memory\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create a memory component\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    # Create a conversational agent\n",
    "    conversational_agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Conversational agent with memory initialized!\")\n",
    "    print(\"üí¨ Testing conversation flow...\")\n",
    "\n",
    "    # Start a conversation\n",
    "    print(\"\\nüí¨ Starting conversation...\")\n",
    "\n",
    "    try:\n",
    "        # First message\n",
    "        response1 = conversational_agent.run(\"Hi! I'm planning a trip. Can you help me?\")\n",
    "        print(f\"ü§ñ Agent: {response1}\")\n",
    "        \n",
    "        # Second message (agent should remember context)\n",
    "        response2 = conversational_agent.run(\"What's the weather like in Tokyo?\")\n",
    "        print(f\"ü§ñ Agent: {response2}\")\n",
    "        \n",
    "        # Third message (testing memory)\n",
    "        response3 = conversational_agent.run(\"And what about the weather in London?\")\n",
    "        print(f\"ü§ñ Agent: {response3}\")\n",
    "        \n",
    "        # Fourth message (testing tool combination)\n",
    "        response4 = conversational_agent.run(\"If I have 500 dollars and spend 120 on flights, how much do I have left?\")\n",
    "        print(f\"ü§ñ Agent: {response4}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in conversation: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # üéØ PART 2: Hands-On Exercise\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ PART 2: Hands-On Exercise\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "üéØ Your Turn! Try these exercises:\n",
    "\n",
    "1. **Tool Testing**: Ask the agent to use different tools\n",
    "   - \"Calculate 25 * 4 / 2\"\n",
    "   - \"What's the weather in Sydney?\"\n",
    "   - \"What time is it?\"\n",
    "\n",
    "2. **Multi-Step Tasks**: Give the agent complex requests\n",
    "   - \"If I have $1000 and spend $300 on a hotel, $150 on food, how much do I have left?\"\n",
    "   - \"What's the weather in New York and what time is it there?\"\n",
    "\n",
    "3. **Conversation Flow**: Have a natural conversation\n",
    "   - Ask about planning a weekend trip\n",
    "   - Request calculations for a budget\n",
    "   - Ask about weather in different cities\n",
    "\n",
    "4. **Custom Tool**: Try creating your own simple tool\n",
    "   - A tool that returns random facts\n",
    "   - A tool that converts units\n",
    "   - A tool that gives motivational quotes\n",
    "\n",
    "üí° Tips:\n",
    "- Be specific in your requests\n",
    "- Let the agent use multiple tools when needed\n",
    "- Watch how it reasons through complex tasks\n",
    "- Don't worry if it makes mistakes - this is learning!\n",
    "\"\"\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # üîç PART 2: Agent Reasoning Demonstration\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç PART 2: Agent Reasoning Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "üîç Let's see how the agent thinks through a complex task:\n",
    "\n",
    "Task: \"I have $500 and want to plan a weekend trip. \n",
    "I need to spend $200 on accommodation, $100 on food, \n",
    "and I want to know how much I'll have left for activities.\"\n",
    "\n",
    "The agent should:\n",
    "1. Use the calculator to subtract expenses: $500 - $200 - $100\n",
    "2. Give you the remaining budget\n",
    "3. Suggest what you could do with the remaining money\n",
    "\n",
    "This shows the agent's ability to:\n",
    "- Break down complex requests\n",
    "- Use multiple tools\n",
    "- Provide helpful, contextual responses\n",
    "\"\"\")\n",
    "\n",
    "    # Test the complex task\n",
    "    print(\"\\nüß™ Testing Complex Task...\")\n",
    "    try:\n",
    "        complex_response = conversational_agent.run(\n",
    "            \"I have $500 and want to plan a weekend trip. \"\n",
    "            \"I need to spend $200 on accommodation, $100 on food, \"\n",
    "            \"and I want to know how much I'll have left for activities.\"\n",
    "        )\n",
    "        print(f\"ü§ñ Agent Response: {complex_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # üìù PART 2 SUMMARY\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìù PART 2 SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\"\"\n",
    "‚úÖ What We Accomplished:\n",
    "- Created custom tools (calculator, weather, time)\n",
    "- Built a tool-calling agent with LangChain\n",
    "- Created a conversational agent with memory\n",
    "- Saw how agents reason through complex tasks\n",
    "- Tested multi-step problem solving\n",
    "\n",
    "üîë Key Concepts:\n",
    "- Tools: Functions agents can call\n",
    "- Agents: LLMs that can use tools\n",
    "- Memory: How agents remember conversations\n",
    "- Reasoning: How agents break down complex tasks\n",
    "\n",
    "üöÄ Next Up: Building Your Own MCP Server!\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\\nüéâ Part 2 Complete! Ready for Part 3: MCP Server Development?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6313022",
   "metadata": {},
   "outputs": [],
   "source": [
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  LangChain not available: {e}\")\n",
    "    print(\"üí° Install with: pip install langchain langchain-openai\")\n",
    "    print(\"üöÄ Continuing to Part 3...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35f6f7",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîå PART 3: MCP SERVER DEVELOPMENT\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9daa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîå PART 3: MCP Server Development\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Understanding MCP (Model Context Protocol)\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üîç MCP (Model Context Protocol) is a way for AI models to:\n",
    "- Connect to external tools and resources\n",
    "- Access real-time information\n",
    "- Interact with your local system\n",
    "- Extend their capabilities beyond just text\n",
    "\n",
    "üí° Think of it as giving your AI a \"remote control\" to your computer!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f830af",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üõ†Ô∏è PART 3: Installing MCP Requirements\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56f027",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üõ†Ô∏è PART 3: Installing MCP Requirements\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996c273",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def install_mcp_requirements():\n",
    "    \"\"\"Install required packages for MCP development\"\"\"\n",
    "    packages = [\n",
    "        \"mcp\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üì¶ Installing MCP packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ùå Failed to install {package}\")\n",
    "            print(\"üí° You may need to install manually: pip install mcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511b08e",
   "metadata": {},
   "source": [
    "Uncomment the line below to install packages\n",
    "install_mcp_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° If you haven't installed MCP packages yet, run:\")\n",
    "print(\"   pip install mcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514a609",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üöÄ PART 3: Creating Your First MCP Server\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ PART 3: Creating Your First MCP Server\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MCP server\n",
    "mcp_server_code = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üéØ Simple MCP Server - Hello World\n",
    "This server provides basic tools for demonstration\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "class SimpleMCPServer:\n",
    "    \"\"\"Simple MCP Server implementation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools = {\n",
    "            \"hello_world\": {\n",
    "                \"description\": \"A simple hello world tool that greets the user\",\n",
    "                \"inputSchema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Your name (optional)\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"get_current_time\": {\n",
    "                \"description\": \"Get the current date and time\",\n",
    "                \"inputSchema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {}\n",
    "                }\n",
    "            },\n",
    "            \"calculate\": {\n",
    "                \"description\": \"Perform basic mathematical calculations\",\n",
    "                \"inputSchema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Mathematical expression to evaluate\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    async def handle_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle incoming MCP requests\"\"\"\n",
    "        \n",
    "        method = request.get(\"method\")\n",
    "        request_id = request.get(\"id\")\n",
    "        \n",
    "        if method == \"initialize\":\n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"result\": {\n",
    "                    \"protocolVersion\": \"2024-11-05\",\n",
    "                    \"capabilities\": {\n",
    "                        \"tools\": {}\n",
    "                    },\n",
    "                    \"serverInfo\": {\n",
    "                        \"name\": \"workshop-mcp-server\",\n",
    "                        \"version\": \"1.0.0\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        elif method == \"tools/list\":\n",
    "            tools_list = []\n",
    "            for name, tool_info in self.tools.items():\n",
    "                tools_list.append({\n",
    "                    \"name\": name,\n",
    "                    \"description\": tool_info[\"description\"],\n",
    "                    \"inputSchema\": tool_info[\"inputSchema\"]\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"result\": {\n",
    "                    \"tools\": tools_list\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        elif method == \"tools/call\":\n",
    "            tool_name = request.get(\"params\", {}).get(\"name\")\n",
    "            arguments = request.get(\"params\", {}).get(\"arguments\", {})\n",
    "            \n",
    "            result = await self.execute_tool(tool_name, arguments)\n",
    "            \n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"result\": {\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": result\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"error\": {\n",
    "                    \"code\": -32601,\n",
    "                    \"message\": f\"Method not found: {method}\"\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    async def execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute the requested tool\"\"\"\n",
    "        \n",
    "        if tool_name == \"hello_world\":\n",
    "            name = arguments.get(\"name\", \"there\")\n",
    "            return f\"Hello {name}! üëã Welcome to your first MCP server!\"\n",
    "        \n",
    "        elif tool_name == \"get_current_time\":\n",
    "            now = datetime.now()\n",
    "            return f\"üïê Current time: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        \n",
    "        elif tool_name == \"calculate\":\n",
    "            expression = arguments.get(\"expression\", \"\")\n",
    "            try:\n",
    "                # Simple and safe evaluation for basic math\n",
    "                allowed_chars = set('0123456789+-*/.() ')\n",
    "                if not all(c in allowed_chars for c in expression):\n",
    "                    return \"‚ùå Error: Only basic mathematical expressions are allowed\"\n",
    "                \n",
    "                result = eval(expression)\n",
    "                return f\"üßÆ {expression} = {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"‚ùå Error calculating '{expression}': {str(e)}\"\n",
    "        \n",
    "        else:\n",
    "            return f\"‚ùå Unknown tool: {tool_name}\"\n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"Run the MCP server\"\"\"\n",
    "        print(\"üöÄ Starting Simple MCP Server...\", file=sys.stderr)\n",
    "        print(\"üìã Available tools:\", file=sys.stderr)\n",
    "        for name, tool_info in self.tools.items():\n",
    "            print(f\"   - {name}: {tool_info['description']}\", file=sys.stderr)\n",
    "        print(\"üîå Server ready for connections!\", file=sys.stderr)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Read request from stdin\n",
    "                line = await asyncio.get_event_loop().run_in_executor(None, sys.stdin.readline)\n",
    "                if not line:\n",
    "                    break\n",
    "                \n",
    "                request = json.loads(line.strip())\n",
    "                response = await self.handle_request(request)\n",
    "                \n",
    "                # Send response to stdout\n",
    "                print(json.dumps(response))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\", file=sys.stderr)\n",
    "                break\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    server = SimpleMCPServer()\n",
    "    await server.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ef8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MCP server code\n",
    "with open(\"workshop_mcp_server.py\", \"w\") as f:\n",
    "    f.write(mcp_server_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b328f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Created workshop_mcp_server.py\")\n",
    "print(\"üìã This server provides 3 simple tools:\")\n",
    "print(\"   - hello_world: Greet the user\")\n",
    "print(\"   - get_current_time: Get current time\")\n",
    "print(\"   - calculate_simple: Do math calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65925438",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üß™ PART 3: Testing Your MCP Server\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß™ PART 3: Testing Your MCP Server\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üß™ To test your MCP server:\n",
    "\n",
    "1. **Start the server** (in a new terminal):\n",
    "   python workshop_mcp_server.py\n",
    "\n",
    "2. **Test with curl** (in another terminal):\n",
    "   curl -X POST http://localhost:8000/call_tool \\\\\n",
    "        -H \"Content-Type: application/json\" \\\\\n",
    "        -d '{\"name\": \"hello_world\", \"arguments\": {\"name\": \"Alice\"}}'\n",
    "\n",
    "3. **Or use the MCP client**:\n",
    "   python -c \"\n",
    "   from mcp.client.stdio import stdio_client\n",
    "   import asyncio\n",
    "   \n",
    "   async def test():\n",
    "       async with stdio_client(['python', 'workshop_mcp_server.py']) as client:\n",
    "           tools = await client.list_tools()\n",
    "           print('Available tools:', tools)\n",
    "   \n",
    "   asyncio.run(test())\n",
    "   \"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f538a9",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîå PART 3: Connecting from VS Code/Cursor\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîå PART 3: Connecting from VS Code/Cursor\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4acdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üîå To connect your MCP server to VS Code/Cursor:\n",
    "\n",
    "1. **Install MCP Extension**:\n",
    "   - VS Code: Search for \"MCP\" in extensions\n",
    "   - Cursor: Should have MCP support built-in\n",
    "\n",
    "2. **Configure MCP Client**:\n",
    "   Create ~/.config/mcp/clients.json:\n",
    "   {\n",
    "     \"workshop-server\": {\n",
    "       \"command\": \"python\",\n",
    "       \"args\": [\"/path/to/your/workshop_mcp_server.py\"],\n",
    "       \"env\": {}\n",
    "     }\n",
    "   }\n",
    "\n",
    "3. **Restart VS Code/Cursor**:\n",
    "   - The MCP server should now be available\n",
    "   - You can ask the AI to use your custom tools!\n",
    "\n",
    "4. **Test the Connection**:\n",
    "   Ask: \"What time is it?\" or \"Calculate 15 * 3\"\n",
    "   The AI should use your MCP server tools!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b29b2",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéØ PART 3: Hands-On Exercise\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ PART 3: Hands-On Exercise\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746398e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üéØ Your Turn! Try these exercises:\n",
    "\n",
    "1. **Run Your Server**:\n",
    "   - Start the MCP server in a terminal\n",
    "   - Test it with simple requests\n",
    "   - Make sure it responds correctly\n",
    "\n",
    "2. **Add a New Tool**:\n",
    "   - Create a tool that returns random facts\n",
    "   - Add a tool that converts temperatures\n",
    "   - Build a tool that gives motivational quotes\n",
    "\n",
    "3. **Connect to VS Code/Cursor**:\n",
    "   - Set up the MCP client configuration\n",
    "   - Restart your editor\n",
    "   - Ask the AI to use your tools\n",
    "\n",
    "4. **Debug and Improve**:\n",
    "   - Add error handling to your tools\n",
    "   - Improve the tool descriptions\n",
    "   - Add input validation\n",
    "\n",
    "üí° Tips:\n",
    "- Start simple and build up\n",
    "- Test each tool individually\n",
    "- Check the MCP documentation for more features\n",
    "- Don't worry about perfection - focus on learning!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090de2b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üîç PART 3: Advanced MCP Features\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç PART 3: Advanced MCP Features\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üîç Once you're comfortable with basics, explore:\n",
    "\n",
    "1. **Resources**: Provide access to files, databases, APIs\n",
    "2. **Prompts**: Create reusable prompt templates\n",
    "3. **Logging**: Add structured logging to your server\n",
    "4. **Authentication**: Secure your server with proper auth\n",
    "5. **Streaming**: Handle long-running operations\n",
    "6. **Error Handling**: Graceful error handling and recovery\n",
    "\n",
    "üöÄ **Real-World Examples**:\n",
    "- File system access (read/write files)\n",
    "- Database queries (SQL, NoSQL)\n",
    "- API integrations (weather, news, etc.)\n",
    "- System monitoring (CPU, memory, processes)\n",
    "- Custom business logic tools\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2e3df",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üìù PART 3 SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fe680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù PART 3 SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚úÖ What We Accomplished:\n",
    "- Learned what MCP is and why it's useful\n",
    "- Created a simple MCP server with 3 tools\n",
    "- Understood how to connect from VS Code/Cursor\n",
    "- Explored advanced MCP capabilities\n",
    "\n",
    "üîë Key Concepts:\n",
    "- MCP Server: Provides tools and resources\n",
    "- MCP Client: Connects to servers (VS Code, etc.)\n",
    "- Tools: Functions the AI can call\n",
    "- Resources: Data the AI can access\n",
    "\n",
    "üöÄ Next Steps:\n",
    "- Run your server and test it\n",
    "- Connect it to VS Code/Cursor\n",
    "- Build more sophisticated tools\n",
    "- Explore the MCP ecosystem\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f90d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéâ Part 3 Complete! You now have a working MCP server!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596e738",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "üéâ WORKSHOP COMPLETE!\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ WORKSHOP COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üåü **CONGRATULATIONS!** üåü\n",
    "\n",
    "You've completed the complete LLM workshop and now have:\n",
    "\n",
    "‚úÖ A working connection to your LLM endpoint  \n",
    "‚úÖ Experience with LangChain agents and tools  \n",
    "‚úÖ Your own MCP server with custom tools  \n",
    "‚úÖ Knowledge of how to integrate everything  \n",
    "\n",
    "**What You've Built:**\n",
    "1. **LLM Connection**: Direct API access to your endpoint\n",
    "2. **Custom Tools**: Calculator, weather, time tools\n",
    "3. **Smart Agents**: Agents that can reason and use tools\n",
    "4. **MCP Server**: Your own server that VS Code/Cursor can use\n",
    "\n",
    "**Next Steps:**\n",
    "- Customize your tools and agents\n",
    "- Build more sophisticated MCP servers\n",
    "- Integrate with real APIs and databases\n",
    "- Create your own AI-powered applications\n",
    "\n",
    "**Time to build something amazing! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "**Workshop Files Created:**\n",
    "- `workshop_mcp_server.py` - Your working MCP server\n",
    "- All workshop code is now in Jupiter notebook format\n",
    "\n",
    "**Resources:**\n",
    "- Check README.md for detailed instructions\n",
    "- Use QUICK_START.md for quick reference\n",
    "- Explore the individual notebook parts for focused learning\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ **Happy coding and building!** üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
