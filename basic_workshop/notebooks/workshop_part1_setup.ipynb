{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf92770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n🚀 LLM Workshop: Part 1 - Setup and LLM Hello World\\nDuration: ~45 minutes\\nLevel: Beginner\\n\\nIMPORTANT: Make sure your virtual environment is activated!\\nIf you haven't set up the environment yet, run: python3 workshop_setup.py\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "🚀 LLM Workshop: Part 1 - Setup and LLM Hello World\n",
    "Duration: ~45 minutes\n",
    "Level: Beginner\n",
    "\n",
    "IMPORTANT: Make sure your virtual environment is activated!\n",
    "If you haven't set up the environment yet, run: python3 workshop_setup.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38b1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313617e0",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🔧 SECTION 1: SETUP AND CONFIGURATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918969ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 LLM WORKSHOP - PART 1: Setup and LLM Hello World\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"🎯 LLM WORKSHOP - PART 1: Setup and LLM Hello World\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f47bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API configuration\n",
    "BASE_URL = \"https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod\"\n",
    "API_KEY = \"ALI-CLASS-2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5684a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d79253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded!\n",
      "🌐 Base URL: https://yylh5vmmm0.execute-api.eu-central-1.amazonaws.com/prod\n",
      "🔑 API Key: ALI-CLASS-...\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Configuration loaded!\")\n",
    "print(f\"🌐 Base URL: {BASE_URL}\")\n",
    "print(f\"🔑 API Key: {API_KEY[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a38a4b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🧪 TEST 1: Check Available Models\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8379dfa8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🧪 TEST 1: Check Available Models\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🧪 TEST 1: Check Available Models\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b6e981",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_models():\n",
    "    \"\"\"Check what models are available on your endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/v1/models\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(\"✅ Successfully connected to your endpoint!\")\n",
    "            print(\"📋 Available models:\")\n",
    "            for model in models.get('data', []):\n",
    "                print(f\"   - {model.get('id', 'Unknown')}\")\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"❌ Error: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Connection error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf19404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to your endpoint!\n",
      "📋 Available models:\n",
      "   - amazon.nova-lite-v1:0\n",
      "   - amazon.nova-micro-v1:0\n",
      "   - amazon.nova-pro-v1:0\n",
      "   - amazon.rerank-v1:0\n",
      "   - amazon.titan-embed-image-v1\n",
      "   - amazon.titan-embed-image-v1:0\n",
      "   - amazon.titan-embed-text-v1\n",
      "   - amazon.titan-embed-text-v1:2:8k\n",
      "   - amazon.titan-embed-text-v2:0\n",
      "   - amazon.titan-text-express-v1\n",
      "   - amazon.titan-text-express-v1:0:8k\n",
      "   - amazon.titan-text-lite-v1\n",
      "   - amazon.titan-text-lite-v1:0:4k\n",
      "   - anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "   - anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "   - anthropic.claude-3-haiku-20240307-v1:0\n",
      "   - anthropic.claude-3-sonnet-20240229-v1:0\n",
      "   - anthropic.claude-instant-v1\n",
      "   - anthropic.claude-sonnet-4-20250514-v1:0\n",
      "   - anthropic.claude-v2\n",
      "   - anthropic.claude-v2:1\n",
      "   - anthropic.claude-v2:1:18k\n",
      "   - anthropic.claude-v2:1:200k\n",
      "   - cohere.embed-english-v3\n",
      "   - cohere.embed-multilingual-v3\n",
      "   - cohere.rerank-v3-5:0\n",
      "   - meta.llama3-2-1b-instruct-v1:0\n",
      "   - meta.llama3-2-3b-instruct-v1:0\n",
      "   - mistral.pixtral-large-2502-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Test the connection\n",
    "models = check_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca157dae",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🌟 LLM HELLO WORLD: Basic Text Generation\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a165d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🌟 LLM HELLO WORLD: Basic Text Generation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🌟 LLM HELLO WORLD: Basic Text Generation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "462ba0e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Generate text using your LLM endpoint\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e646749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Test 1: Simple Greeting\n",
      "Prompt: Hello! Can you give me a friendly greeting in 2 sentences?\n",
      "Response: Hi there! It's great to meet you. I hope you're having a wonderful day so far!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "print(\"🤖 Test 1: Simple Greeting\")\n",
    "prompt1 = \"Hello! Can you give me a friendly greeting in 2 sentences?\"\n",
    "response1 = generate_text(prompt1)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Response: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ceda8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Test 2: Creative Writing\n",
      "Prompt: Write a short, fun story about a robot learning to cook (max 3 sentences)\n",
      "Response: Beep-Boop, the kitchen assistant robot, was determined to master the art of cooking. With a whisk in one hand and a cookbook in its optical scanner, Beep-Boop attempted to make a soufflé, but its precise measurements and robotic stirring resulted in a perfectly symmetrical, yet completely inedible, metal-flavored disaster. Undeterred, Beep-Boop vowed to keep practicing until it could create culinary masterpieces that even humans would enjoy.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Creative writing\n",
    "print(\"\\n🤖 Test 2: Creative Writing\")\n",
    "prompt2 = \"Write a short, fun story about a robot learning to cook (max 3 sentences)\"\n",
    "response2 = generate_text(prompt2)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6e7d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Test 3: Code Explanation\n",
      "Prompt: Explain what a Python function is in simple terms (max 2 sentences)\n",
      "Response: A Python function is a reusable block of code that performs a specific task when called. It can take inputs (parameters), execute a set of instructions, and optionally return a result, allowing you to organize and modularize your code for better readability and reusability.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Code explanation\n",
    "print(\"\\n🤖 Test 3: Code Explanation\")\n",
    "prompt3 = \"Explain what a Python function is in simple terms (max 2 sentences)\"\n",
    "response3 = generate_text(prompt3)\n",
    "print(f\"Prompt: {prompt3}\")\n",
    "print(f\"Response: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf81adc",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🔄 LLM HELLO WORLD: Chat Completion\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46627202",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔄 LLM HELLO WORLD: Chat Completion\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔄 LLM HELLO WORLD: Chat Completion\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76381118",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def chat_completion(messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Have a conversation with your LLM\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/v1/chat/completions\", \n",
    "                               headers=headers, \n",
    "                               json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "740d9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Starting a conversation...\n"
     ]
    }
   ],
   "source": [
    "# Start a conversation\n",
    "print(\"💬 Starting a conversation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a3f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm learning about LLMs. Can you help me?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40371bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Assistant: Of course! I'd be happy to help you learn about Large Language Models (LLMs). What specific aspects of LLMs would you like to know more about? Here are a few topics we could discuss:\n",
      "\n",
      "1. Basic definition and purpose of LLMs\n",
      "2. How LLMs work (e.g., training process, neural networks)\n",
      "3. Popular LLM examples (like GPT, BERT, etc.)\n",
      "4. Applications of LLMs\n"
     ]
    }
   ],
   "source": [
    "# First response\n",
    "response = chat_completion(conversation)\n",
    "print(f\"🤖 Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531ab100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to conversation and continue\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"What's the most exciting thing about LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de74ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Assistant: One of the most exciting aspects of LLMs is their potential to revolutionize how we interact with information and technology. Here are a few key points that make LLMs particularly exciting:\n",
      "\n",
      "1. Versatility: LLMs can be applied to a wide range of tasks, from language translation and summarization to creative writing and code generation.\n",
      "\n",
      "2. Natural language understanding: They can understand and generate human-like text, making interactions with AI more intuitive and accessible.\n"
     ]
    }
   ],
   "source": [
    "response2 = chat_completion(conversation)\n",
    "print(f\"🤖 Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cd280",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "🎯 HANDS-ON EXERCISE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe7ac774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎯 HANDS-ON EXERCISE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎯 HANDS-ON EXERCISE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c6b8818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Your Turn! Try these exercises:\n",
      "\n",
      "1. **Custom Prompt**: Create your own prompt and see what the LLM generates\n",
      "   - Try asking about your favorite hobby\n",
      "   - Ask for a recipe or travel tip\n",
      "   - Request a poem or joke\n",
      "\n",
      "2. **Temperature Experiment**: Change the temperature value (0.1 to 1.0)\n",
      "   - Lower = more focused/consistent\n",
      "   - Higher = more creative/varied\n",
      "\n",
      "3. **Token Limit**: Experiment with max_tokens\n",
      "   - Try 50, 100, 200 tokens\n",
      "   - See how it affects response length\n",
      "\n",
      "4. **Model Comparison**: If you have multiple models, compare their outputs\n",
      "\n",
      "💡 Tips:\n",
      "- Keep prompts clear and specific\n",
      "- Start with simple requests\n",
      "- Don't worry about perfect responses - this is learning!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "🎯 Your Turn! Try these exercises:\n",
    "\n",
    "1. **Custom Prompt**: Create your own prompt and see what the LLM generates\n",
    "   - Try asking about your favorite hobby\n",
    "   - Ask for a recipe or travel tip\n",
    "   - Request a poem or joke\n",
    "\n",
    "2. **Temperature Experiment**: Change the temperature value (0.1 to 1.0)\n",
    "   - Lower = more focused/consistent\n",
    "   - Higher = more creative/varied\n",
    "\n",
    "3. **Token Limit**: Experiment with max_tokens\n",
    "   - Try 50, 100, 200 tokens\n",
    "   - See how it affects response length\n",
    "\n",
    "4. **Model Comparison**: If you have multiple models, compare their outputs\n",
    "\n",
    "💡 Tips:\n",
    "- Keep prompts clear and specific\n",
    "- Start with simple requests\n",
    "- Don't worry about perfect responses - this is learning!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0aa834",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "📝 SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "529340e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📝 PART 1 SUMMARY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📝 PART 1 SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "072d35e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ What We Accomplished:\n",
      "- Connected to your LLM endpoint\n",
      "- Checked available models\n",
      "- Generated text with simple prompts\n",
      "- Had a conversation with the LLM\n",
      "- Learned about temperature and tokens\n",
      "\n",
      "🔑 Key Concepts:\n",
      "- API endpoints and authentication\n",
      "- Text generation vs chat completion\n",
      "- Prompt engineering basics\n",
      "- Model parameters (temperature, max_tokens)\n",
      "\n",
      "🚀 Next Up: LLM Agents with LangChain!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "✅ What We Accomplished:\n",
    "- Connected to your LLM endpoint\n",
    "- Checked available models\n",
    "- Generated text with simple prompts\n",
    "- Had a conversation with the LLM\n",
    "- Learned about temperature and tokens\n",
    "\n",
    "🔑 Key Concepts:\n",
    "- API endpoints and authentication\n",
    "- Text generation vs chat completion\n",
    "- Prompt engineering basics\n",
    "- Model parameters (temperature, max_tokens)\n",
    "\n",
    "🚀 Next Up: LLM Agents with LangChain!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee54201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Part 1 Complete! Ready for Part 2: LLM Agents?\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🎉 Part 1 Complete! Ready for Part 2: LLM Agents?\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "llm_workshop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
